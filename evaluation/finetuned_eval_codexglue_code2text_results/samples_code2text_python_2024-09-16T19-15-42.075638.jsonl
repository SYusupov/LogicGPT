{"doc_id": 0, "doc": {"id": 0, "repo": "soimort/you-get", "path": "src/you_get/extractors/miomio.py", "func_name": "sina_xml_to_url_list", "original_string": "def sina_xml_to_url_list(xml_data):\n    \"\"\"str->list\n    Convert XML to URL List.\n    From Biligrab.\n    \"\"\"\n    rawurl = []\n    dom = parseString(xml_data)\n    for node in dom.getElementsByTagName('durl'):\n        url = node.getElementsByTagName('url')[0]\n        rawurl.append(url.childNodes[0].data)\n    return rawurl", "language": "python", "code": "def sina_xml_to_url_list(xml_data):\n    \"\"\"str->list\n    Convert XML to URL List.\n    From Biligrab.\n    \"\"\"\n    rawurl = []\n    dom = parseString(xml_data)\n    for node in dom.getElementsByTagName('durl'):\n        url = node.getElementsByTagName('url')[0]\n        rawurl.append(url.childNodes[0].data)\n    return rawurl", "code_tokens": ["def", "sina_xml_to_url_list", "(", "xml_data", ")", ":", "rawurl", "=", "[", "]", "dom", "=", "parseString", "(", "xml_data", ")", "for", "node", "in", "dom", ".", "getElementsByTagName", "(", "'durl'", ")", ":", "url", "=", "node", ".", "getElementsByTagName", "(", "'url'", ")", "[", "0", "]", "rawurl", ".", "append", "(", "url", ".", "childNodes", "[", "0", "]", ".", "data", ")", "return", "rawurl"], "docstring": "str->list\n    Convert XML to URL List.\n    From Biligrab.", "docstring_tokens": ["str", "-", ">", "list", "Convert", "XML", "to", "URL", "List", ".", "From", "Biligrab", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/miomio.py#L41-L51"}, "target": "str - > list Convert XML to URL List . From Biligrab .", "arguments": {"gen_args_0": {"arg_0": "def sina_xml_to_url_list ( xml_data ) : rawurl = [ ] dom = parseString ( xml_data ) for node in dom . getElementsByTagName ( 'durl' ) : url = node . getElementsByTagName ( 'url' ) [ 0 ] rawurl . append ( url . childNodes [ 0 ] . data ) return rawurl", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def sina_xml_to_title_list ( xml_"]], "filtered_resps": ["def sina_xml_to_title_list ( xml_"], "doc_hash": "8d8fc051ec2d4b20dabc8d03f16cfd346d76e1ca408c1e23dd630077961dda85", "prompt_hash": "ac395884a05a3ecc075c28fc3c39653a4586f8347067e15b5b8532babd99e196", "target_hash": "ab30db85997b4505763d2bccf16dd82e9f218f1e40e5393657672462a35818e9", "smoothed_bleu_4": 10.768746526902671}
{"doc_id": 1, "doc": {"id": 1, "repo": "soimort/you-get", "path": "src/you_get/extractors/dailymotion.py", "func_name": "dailymotion_download", "original_string": "def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Dailymotion videos by URL.\n    \"\"\"\n\n    html = get_content(rebuilt_url(url))\n    info = json.loads(match1(html, r'qualities\":({.+?}),\"'))\n    title = match1(html, r'\"video_title\"\\s*:\\s*\"([^\"]+)\"') or \\\n            match1(html, r'\"title\"\\s*:\\s*\"([^\"]+)\"')\n    title = unicodize(title)\n\n    for quality in ['1080','720','480','380','240','144','auto']:\n        try:\n            real_url = info[quality][1][\"url\"]\n            if real_url:\n                break\n        except KeyError:\n            pass\n\n    mime, ext, size = url_info(real_url)\n\n    print_info(site_info, title, mime, size)\n    if not info_only:\n        download_urls([real_url], title, ext, size, output_dir=output_dir, merge=merge)", "language": "python", "code": "def dailymotion_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Dailymotion videos by URL.\n    \"\"\"\n\n    html = get_content(rebuilt_url(url))\n    info = json.loads(match1(html, r'qualities\":({.+?}),\"'))\n    title = match1(html, r'\"video_title\"\\s*:\\s*\"([^\"]+)\"') or \\\n            match1(html, r'\"title\"\\s*:\\s*\"([^\"]+)\"')\n    title = unicodize(title)\n\n    for quality in ['1080','720','480','380','240','144','auto']:\n        try:\n            real_url = info[quality][1][\"url\"]\n            if real_url:\n                break\n        except KeyError:\n            pass\n\n    mime, ext, size = url_info(real_url)\n\n    print_info(site_info, title, mime, size)\n    if not info_only:\n        download_urls([real_url], title, ext, size, output_dir=output_dir, merge=merge)", "code_tokens": ["def", "dailymotion_download", "(", "url", ",", "output_dir", "=", "'.'", ",", "merge", "=", "True", ",", "info_only", "=", "False", ",", "*", "*", "kwargs", ")", ":", "html", "=", "get_content", "(", "rebuilt_url", "(", "url", ")", ")", "info", "=", "json", ".", "loads", "(", "match1", "(", "html", ",", "r'qualities\":({.+?}),\"'", ")", ")", "title", "=", "match1", "(", "html", ",", "r'\"video_title\"\\s*:\\s*\"([^\"]+)\"'", ")", "or", "match1", "(", "html", ",", "r'\"title\"\\s*:\\s*\"([^\"]+)\"'", ")", "title", "=", "unicodize", "(", "title", ")", "for", "quality", "in", "[", "'1080'", ",", "'720'", ",", "'480'", ",", "'380'", ",", "'240'", ",", "'144'", ",", "'auto'", "]", ":", "try", ":", "real_url", "=", "info", "[", "quality", "]", "[", "1", "]", "[", "\"url\"", "]", "if", "real_url", ":", "break", "except", "KeyError", ":", "pass", "mime", ",", "ext", ",", "size", "=", "url_info", "(", "real_url", ")", "print_info", "(", "site_info", ",", "title", ",", "mime", ",", "size", ")", "if", "not", "info_only", ":", "download_urls", "(", "[", "real_url", "]", ",", "title", ",", "ext", ",", "size", ",", "output_dir", "=", "output_dir", ",", "merge", "=", "merge", ")"], "docstring": "Downloads Dailymotion videos by URL.", "docstring_tokens": ["Downloads", "Dailymotion", "videos", "by", "URL", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/dailymotion.py#L13-L35"}, "target": "Downloads Dailymotion videos by URL .", "arguments": {"gen_args_0": {"arg_0": "def dailymotion_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : html = get_content ( rebuilt_url ( url ) ) info = json . loads ( match1 ( html , r'qualities\":({.+?}),\"' ) ) title = match1 ( html , r'\"video_title\"\\s*:\\s*\"([^\"]+)\"' ) or match1 ( html , r'\"title\"\\s*:\\s*\"([^\"]+)\"' ) title = unicodize ( title ) for quality in [ '1080' , '720' , '480' , '380' , '240' , '144' , 'auto' ] : try : real_url = info [ quality ] [ 1 ] [ \"url\" ] if real_url : break except KeyError : pass mime , ext , size = url_info ( real_url ) print_info ( site_info , title , mime , size ) if not info_only : download_urls ( [ real_url ] , title , ext , size , output_dir = output_dir , merge = merge )", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["return"]], "filtered_resps": ["return"], "doc_hash": "66e524bd9e58dd1853f7fd335fc11476d03630f45e91970e8ef3ec0c3b3fa7be", "prompt_hash": "d70e2ac8065d724ccbf851452f5814a688179d5122c4416153e1d4aa67475da9", "target_hash": "6d57a4076399520800871e519563a62ae6ad5048b8178f01b4d4009b711d1823", "smoothed_bleu_4": 1.0025358302619216e-76}
{"doc_id": 2, "doc": {"id": 2, "repo": "soimort/you-get", "path": "src/you_get/extractors/sina.py", "func_name": "sina_download", "original_string": "def sina_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Sina videos by URL.\n    \"\"\"\n    if 'news.sina.com.cn/zxt' in url:\n        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n        return\n\n    vid = match1(url, r'vid=(\\d+)')\n    if vid is None:\n        video_page = get_content(url)\n        vid = hd_vid = match1(video_page, r'hd_vid\\s*:\\s*\\'([^\\']+)\\'')\n        if hd_vid == '0':\n            vids = match1(video_page, r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'').split('|')\n            vid = vids[-1]\n\n    if vid is None:\n        vid = match1(video_page, r'vid:\"?(\\d+)\"?')\n    if vid:\n        #title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'')\n        sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n    else:\n        vkey = match1(video_page, r'vkey\\s*:\\s*\"([^\"]+)\"')\n        if vkey is None:\n            vid = match1(url, r'#(\\d+)')\n            sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n            return\n        title = match1(video_page, r'title\\s*:\\s*\"([^\"]+)\"')\n        sina_download_by_vkey(vkey, title=title, output_dir=output_dir, merge=merge, info_only=info_only)", "language": "python", "code": "def sina_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads Sina videos by URL.\n    \"\"\"\n    if 'news.sina.com.cn/zxt' in url:\n        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n        return\n\n    vid = match1(url, r'vid=(\\d+)')\n    if vid is None:\n        video_page = get_content(url)\n        vid = hd_vid = match1(video_page, r'hd_vid\\s*:\\s*\\'([^\\']+)\\'')\n        if hd_vid == '0':\n            vids = match1(video_page, r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'').split('|')\n            vid = vids[-1]\n\n    if vid is None:\n        vid = match1(video_page, r'vid:\"?(\\d+)\"?')\n    if vid:\n        #title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'')\n        sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n    else:\n        vkey = match1(video_page, r'vkey\\s*:\\s*\"([^\"]+)\"')\n        if vkey is None:\n            vid = match1(url, r'#(\\d+)')\n            sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n            return\n        title = match1(video_page, r'title\\s*:\\s*\"([^\"]+)\"')\n        sina_download_by_vkey(vkey, title=title, output_dir=output_dir, merge=merge, info_only=info_only)", "code_tokens": ["def", "sina_download", "(", "url", ",", "output_dir", "=", "'.'", ",", "merge", "=", "True", ",", "info_only", "=", "False", ",", "*", "*", "kwargs", ")", ":", "if", "'news.sina.com.cn/zxt'", "in", "url", ":", "sina_zxt", "(", "url", ",", "output_dir", "=", "output_dir", ",", "merge", "=", "merge", ",", "info_only", "=", "info_only", ",", "*", "*", "kwargs", ")", "return", "vid", "=", "match1", "(", "url", ",", "r'vid=(\\d+)'", ")", "if", "vid", "is", "None", ":", "video_page", "=", "get_content", "(", "url", ")", "vid", "=", "hd_vid", "=", "match1", "(", "video_page", ",", "r'hd_vid\\s*:\\s*\\'([^\\']+)\\''", ")", "if", "hd_vid", "==", "'0'", ":", "vids", "=", "match1", "(", "video_page", ",", "r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\''", ")", ".", "split", "(", "'|'", ")", "vid", "=", "vids", "[", "-", "1", "]", "if", "vid", "is", "None", ":", "vid", "=", "match1", "(", "video_page", ",", "r'vid:\"?(\\d+)\"?'", ")", "if", "vid", ":", "#title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'')", "sina_download_by_vid", "(", "vid", ",", "output_dir", "=", "output_dir", ",", "merge", "=", "merge", ",", "info_only", "=", "info_only", ")", "else", ":", "vkey", "=", "match1", "(", "video_page", ",", "r'vkey\\s*:\\s*\"([^\"]+)\"'", ")", "if", "vkey", "is", "None", ":", "vid", "=", "match1", "(", "url", ",", "r'#(\\d+)'", ")", "sina_download_by_vid", "(", "vid", ",", "output_dir", "=", "output_dir", ",", "merge", "=", "merge", ",", "info_only", "=", "info_only", ")", "return", "title", "=", "match1", "(", "video_page", ",", "r'title\\s*:\\s*\"([^\"]+)\"'", ")", "sina_download_by_vkey", "(", "vkey", ",", "title", "=", "title", ",", "output_dir", "=", "output_dir", ",", "merge", "=", "merge", ",", "info_only", "=", "info_only", ")"], "docstring": "Downloads Sina videos by URL.", "docstring_tokens": ["Downloads", "Sina", "videos", "by", "URL", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/sina.py#L94-L121"}, "target": "Downloads Sina videos by URL .", "arguments": {"gen_args_0": {"arg_0": "def sina_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : if 'news.sina.com.cn/zxt' in url : sina_zxt ( url , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs ) return vid = match1 ( url , r'vid=(\\d+)' ) if vid is None : video_page = get_content ( url ) vid = hd_vid = match1 ( video_page , r'hd_vid\\s*:\\s*\\'([^\\']+)\\'' ) if hd_vid == '0' : vids = match1 ( video_page , r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'' ) . split ( '|' ) vid = vids [ - 1 ] if vid is None : vid = match1 ( video_page , r'vid:\"?(\\d+)\"?' ) if vid : #title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'') sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) else : vkey = match1 ( video_page , r'vkey\\s*:\\s*\"([^\"]+)\"' ) if vkey is None : vid = match1 ( url , r'#(\\d+)' ) sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) return title = match1 ( video_page , r'title\\s*:\\s*\"([^\"]+)\"' ) sina_download_by_vkey ( vkey , title = title , output_dir = output_dir , merge = merge , info_only = info_only )", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def sina_zxt ( url , output_dir = '.' ,"]], "filtered_resps": ["def sina_zxt ( url , output_dir = '.' ,"], "doc_hash": "14b4b63fe82f9f698d45fd5f664dcb0c0d10c3dee7bd7e1783860ca6ec3d1f8a", "prompt_hash": "9b901f4822d83b5b6ddb9d128fe2c797100acf9210c3eeba71fde7a3d92f2bcb", "target_hash": "2b398a4316ef2ab0ffb3a3bb749562d5b0616a11509919dbae33e67cd5243daa", "smoothed_bleu_4": 9.25159978069645}
{"doc_id": 3, "doc": {"id": 3, "repo": "soimort/you-get", "path": "src/you_get/util/log.py", "func_name": "sprint", "original_string": "def sprint(text, *colors):\n    \"\"\"Format text with color or other effects into ANSI escaped string.\"\"\"\n    return \"\\33[{}m{content}\\33[{}m\".format(\";\".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text", "language": "python", "code": "def sprint(text, *colors):\n    \"\"\"Format text with color or other effects into ANSI escaped string.\"\"\"\n    return \"\\33[{}m{content}\\33[{}m\".format(\";\".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text", "code_tokens": ["def", "sprint", "(", "text", ",", "*", "colors", ")", ":", "return", "\"\\33[{}m{content}\\33[{}m\"", ".", "format", "(", "\";\"", ".", "join", "(", "[", "str", "(", "color", ")", "for", "color", "in", "colors", "]", ")", ",", "RESET", ",", "content", "=", "text", ")", "if", "IS_ANSI_TERMINAL", "and", "colors", "else", "text"], "docstring": "Format text with color or other effects into ANSI escaped string.", "docstring_tokens": ["Format", "text", "with", "color", "or", "other", "effects", "into", "ANSI", "escaped", "string", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L60-L62"}, "target": "Format text with color or other effects into ANSI escaped string .", "arguments": {"gen_args_0": {"arg_0": "def sprint ( text , * colors ) : return \"\\33[{}m{content}\\33[{}m\" . format ( \";\" . join ( [ str ( color ) for color in colors ] ) , RESET , content = text ) if IS_ANSI_TERMINAL and colors else text", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def sprint_color ( text , color ) : return \"\\33"]], "filtered_resps": ["def sprint_color ( text , color ) : return \"\\33"], "doc_hash": "bb999974bd0b3303d71f6c82c99920fabfddec640bd9297f936984788e95bbfc", "prompt_hash": "7fda9d5faec67f1ca624d92e6c928c0a2ae2f838f9f6468f819143e203ca6c02", "target_hash": "c3bdb37a7728f9f98f81973d7711be73ad16d37679f31edca7a98ad4b436eac2", "smoothed_bleu_4": 8.993160234885638}
{"doc_id": 4, "doc": {"id": 4, "repo": "soimort/you-get", "path": "src/you_get/util/log.py", "func_name": "print_log", "original_string": "def print_log(text, *colors):\n    \"\"\"Print a log message to standard error.\"\"\"\n    sys.stderr.write(sprint(\"{}: {}\".format(script_name, text), *colors) + \"\\n\")", "language": "python", "code": "def print_log(text, *colors):\n    \"\"\"Print a log message to standard error.\"\"\"\n    sys.stderr.write(sprint(\"{}: {}\".format(script_name, text), *colors) + \"\\n\")", "code_tokens": ["def", "print_log", "(", "text", ",", "*", "colors", ")", ":", "sys", ".", "stderr", ".", "write", "(", "sprint", "(", "\"{}: {}\"", ".", "format", "(", "script_name", ",", "text", ")", ",", "*", "colors", ")", "+", "\"\\n\"", ")"], "docstring": "Print a log message to standard error.", "docstring_tokens": ["Print", "a", "log", "message", "to", "standard", "error", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L72-L74"}, "target": "Print a log message to standard error .", "arguments": {"gen_args_0": {"arg_0": "def print_log ( text , * colors ) : sys . stderr . write ( sprint ( \"{}: {}\" . format ( script_name , text ) , * colors ) + \"\\n\" )", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def print_error ( text , * colors ) : print_log"]], "filtered_resps": ["def print_error ( text , * colors ) : print_log"], "doc_hash": "a38b93035c9e1bb45aafa85efe2dfc3752a6ce60a8628e4b6dee19698d947335", "prompt_hash": "80a236a05b6b9307c4c01b259d48c1f2165d559b725b0f69c12ba0f19b666c5a", "target_hash": "1654f4cba2e342aff6e6edb2ad05103fd1e65900041b43ccefecf19b2054881a", "smoothed_bleu_4": 9.952567832915834}
{"doc_id": 5, "doc": {"id": 5, "repo": "soimort/you-get", "path": "src/you_get/util/log.py", "func_name": "e", "original_string": "def e(message, exit_code=None):\n    \"\"\"Print an error log message.\"\"\"\n    print_log(message, YELLOW, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)", "language": "python", "code": "def e(message, exit_code=None):\n    \"\"\"Print an error log message.\"\"\"\n    print_log(message, YELLOW, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)", "code_tokens": ["def", "e", "(", "message", ",", "exit_code", "=", "None", ")", ":", "print_log", "(", "message", ",", "YELLOW", ",", "BOLD", ")", "if", "exit_code", "is", "not", "None", ":", "sys", ".", "exit", "(", "exit_code", ")"], "docstring": "Print an error log message.", "docstring_tokens": ["Print", "an", "error", "log", "message", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L88-L92"}, "target": "Print an error log message .", "arguments": {"gen_args_0": {"arg_0": "def e ( message , exit_code = None ) : print_log ( message , YELLOW , BOLD ) if exit_code is not None : sys . exit ( exit_code )", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def w ( message , exit_code = None ) : print_"]], "filtered_resps": ["def w ( message , exit_code = None ) : print_"], "doc_hash": "7d2341dd6927d35fec414b9c496a4c564181b385c14ea2815159fd0ab967b819", "prompt_hash": "0e326f5fb74b859a46efa281176bdbe055033fe70565fdd7dc1714238d437b98", "target_hash": "9286b1246ce486ea06b897a78736b7665f3227e76574aa054c41b156f8b628be", "smoothed_bleu_4": 8.993160234885638}
{"doc_id": 6, "doc": {"id": 6, "repo": "soimort/you-get", "path": "src/you_get/util/log.py", "func_name": "wtf", "original_string": "def wtf(message, exit_code=1):\n    \"\"\"What a Terrible Failure!\"\"\"\n    print_log(message, RED, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)", "language": "python", "code": "def wtf(message, exit_code=1):\n    \"\"\"What a Terrible Failure!\"\"\"\n    print_log(message, RED, BOLD)\n    if exit_code is not None:\n        sys.exit(exit_code)", "code_tokens": ["def", "wtf", "(", "message", ",", "exit_code", "=", "1", ")", ":", "print_log", "(", "message", ",", "RED", ",", "BOLD", ")", "if", "exit_code", "is", "not", "None", ":", "sys", ".", "exit", "(", "exit_code", ")"], "docstring": "What a Terrible Failure!", "docstring_tokens": ["What", "a", "Terrible", "Failure!"], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/log.py#L94-L98"}, "target": "What a Terrible Failure!", "arguments": {"gen_args_0": {"arg_0": "def wtf ( message , exit_code = 1 ) : print_log ( message , RED , BOLD ) if exit_code is not None : sys . exit ( exit_code )", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def print_log ( message , color = WHITE , bold ="]], "filtered_resps": ["def print_log ( message , color = WHITE , bold ="], "doc_hash": "4f93a0d64e2a9711581194e62dfd094041f35e33d5b9a7fffb1aa61d59a0dfee", "prompt_hash": "4e1983ea94d43c9f24bdccd7bd5930df4326b510e4b53928cbc51da9b86982df", "target_hash": "2a29222759e81a59a86e64b4b288c3a9f14534c43d23def25733369921228572", "smoothed_bleu_4": 9.993576674761412e-77}
{"doc_id": 7, "doc": {"id": 7, "repo": "soimort/you-get", "path": "src/you_get/util/os.py", "func_name": "detect_os", "original_string": "def detect_os():\n    \"\"\"Detect operating system.\n    \"\"\"\n\n    # Inspired by:\n    # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py\n\n    syst = system().lower()\n    os = 'unknown'\n\n    if 'cygwin' in syst:\n        os = 'cygwin'\n    elif 'darwin' in syst:\n        os = 'mac'\n    elif 'linux' in syst:\n        os = 'linux'\n        # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423\n        try:\n            with open('/proc/version', 'r') as f:\n                if 'microsoft' in f.read().lower():\n                    os = 'wsl'\n        except: pass\n    elif 'windows' in syst:\n        os = 'windows'\n    elif 'bsd' in syst:\n        os = 'bsd'\n\n    return os", "language": "python", "code": "def detect_os():\n    \"\"\"Detect operating system.\n    \"\"\"\n\n    # Inspired by:\n    # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py\n\n    syst = system().lower()\n    os = 'unknown'\n\n    if 'cygwin' in syst:\n        os = 'cygwin'\n    elif 'darwin' in syst:\n        os = 'mac'\n    elif 'linux' in syst:\n        os = 'linux'\n        # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423\n        try:\n            with open('/proc/version', 'r') as f:\n                if 'microsoft' in f.read().lower():\n                    os = 'wsl'\n        except: pass\n    elif 'windows' in syst:\n        os = 'windows'\n    elif 'bsd' in syst:\n        os = 'bsd'\n\n    return os", "code_tokens": ["def", "detect_os", "(", ")", ":", "# Inspired by:", "# https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py", "syst", "=", "system", "(", ")", ".", "lower", "(", ")", "os", "=", "'unknown'", "if", "'cygwin'", "in", "syst", ":", "os", "=", "'cygwin'", "elif", "'darwin'", "in", "syst", ":", "os", "=", "'mac'", "elif", "'linux'", "in", "syst", ":", "os", "=", "'linux'", "# detect WSL https://github.com/Microsoft/BashOnWindows/issues/423", "try", ":", "with", "open", "(", "'/proc/version'", ",", "'r'", ")", "as", "f", ":", "if", "'microsoft'", "in", "f", ".", "read", "(", ")", ".", "lower", "(", ")", ":", "os", "=", "'wsl'", "except", ":", "pass", "elif", "'windows'", "in", "syst", ":", "os", "=", "'windows'", "elif", "'bsd'", "in", "syst", ":", "os", "=", "'bsd'", "return", "os"], "docstring": "Detect operating system.", "docstring_tokens": ["Detect", "operating", "system", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/os.py#L5-L32"}, "target": "Detect operating system .", "arguments": {"gen_args_0": {"arg_0": "def detect_os ( ) : # Inspired by: # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py syst = system ( ) . lower ( ) os = 'unknown' if 'cygwin' in syst : os = 'cygwin' elif 'darwin' in syst : os = 'mac' elif 'linux' in syst : os = 'linux' # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423 try : with open ( '/proc/version' , 'r' ) as f : if 'microsoft' in f . read ( ) . lower ( ) : os = 'wsl' except : pass elif 'windows' in syst : os = 'windows' elif 'bsd' in syst : os = 'bsd' return os", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [[""]], "filtered_resps": [""], "doc_hash": "c744a9d0fd4688665596a55f02e16020bd2c9cf9b88722617c8c33497129bd2f", "prompt_hash": "c204b1f40f8a4cde147f422767a54d52417b6d0c1a839474dd083935917a4973", "target_hash": "c932dd92d544ce147fc2cdd2b3a797dda804aa12c3cb3207e14268632cc4b573", "smoothed_bleu_4": 1.8315638888734178}
{"doc_id": 8, "doc": {"id": 8, "repo": "soimort/you-get", "path": "src/you_get/extractors/vimeo.py", "func_name": "vimeo_download_by_channel", "original_string": "def vimeo_download_by_channel(url, output_dir='.', merge=False, info_only=False, **kwargs):\n    \"\"\"str->None\"\"\"\n    # https://vimeo.com/channels/464686\n    channel_id = match1(url, r'http://vimeo.com/channels/(\\w+)')\n    vimeo_download_by_channel_id(channel_id, output_dir, merge, info_only, **kwargs)", "language": "python", "code": "def vimeo_download_by_channel(url, output_dir='.', merge=False, info_only=False, **kwargs):\n    \"\"\"str->None\"\"\"\n    # https://vimeo.com/channels/464686\n    channel_id = match1(url, r'http://vimeo.com/channels/(\\w+)')\n    vimeo_download_by_channel_id(channel_id, output_dir, merge, info_only, **kwargs)", "code_tokens": ["def", "vimeo_download_by_channel", "(", "url", ",", "output_dir", "=", "'.'", ",", "merge", "=", "False", ",", "info_only", "=", "False", ",", "*", "*", "kwargs", ")", ":", "# https://vimeo.com/channels/464686", "channel_id", "=", "match1", "(", "url", ",", "r'http://vimeo.com/channels/(\\w+)'", ")", "vimeo_download_by_channel_id", "(", "channel_id", ",", "output_dir", ",", "merge", ",", "info_only", ",", "*", "*", "kwargs", ")"], "docstring": "str->None", "docstring_tokens": ["str", "-", ">", "None"], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/vimeo.py#L15-L19"}, "target": "str - > None", "arguments": {"gen_args_0": {"arg_0": "def vimeo_download_by_channel ( url , output_dir = '.' , merge = False , info_only = False , * * kwargs ) : # https://vimeo.com/channels/464686 channel_id = match1 ( url , r'http://vimeo.com/channels/(\\w+)' ) vimeo_download_by_channel_id ( channel_id , output_dir , merge , info_only , * * kwargs )", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def vimeo_download_by_channel_id ( channel"]], "filtered_resps": ["def vimeo_download_by_channel_id ( channel"], "doc_hash": "fff039119291c22093638496090d32eecc34d3f96038ac546c2374aff366c07a", "prompt_hash": "9ba4a08b2ff95a89bbcc177e697991cf25b0233a76ca63799fdcf68fdced843a", "target_hash": "6f5657478bb64dd636af49223f68acbd89793c75f0ba409d45b689f4a0646568", "smoothed_bleu_4": 1.0886726524416985e-76}
{"doc_id": 9, "doc": {"id": 9, "repo": "soimort/you-get", "path": "src/you_get/extractors/ckplayer.py", "func_name": "ckplayer_get_info_by_xml", "original_string": "def ckplayer_get_info_by_xml(ckinfo):\n    \"\"\"str->dict\n    Information for CKPlayer API content.\"\"\"\n    e = ET.XML(ckinfo)\n    video_dict = {'title': '',\n                  #'duration': 0,\n                  'links': [],\n                  'size': 0,\n                  'flashvars': '',}\n    dictified = dictify(e)['ckplayer']\n    if 'info' in dictified:\n        if '_text' in dictified['info'][0]['title'][0]:  #title\n            video_dict['title'] = dictified['info'][0]['title'][0]['_text'].strip()\n\n    #if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration\n        #video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()\n\n    if '_text' in dictified['video'][0]['size'][0]:  #size exists for 1 piece\n        video_dict['size'] = sum([int(i['size'][0]['_text']) for i in dictified['video']])\n\n    if '_text' in dictified['video'][0]['file'][0]:  #link exist\n        video_dict['links'] = [i['file'][0]['_text'].strip() for i in dictified['video']]\n\n    if '_text' in dictified['flashvars'][0]:\n        video_dict['flashvars'] = dictified['flashvars'][0]['_text'].strip()\n\n    return video_dict", "language": "python", "code": "def ckplayer_get_info_by_xml(ckinfo):\n    \"\"\"str->dict\n    Information for CKPlayer API content.\"\"\"\n    e = ET.XML(ckinfo)\n    video_dict = {'title': '',\n                  #'duration': 0,\n                  'links': [],\n                  'size': 0,\n                  'flashvars': '',}\n    dictified = dictify(e)['ckplayer']\n    if 'info' in dictified:\n        if '_text' in dictified['info'][0]['title'][0]:  #title\n            video_dict['title'] = dictified['info'][0]['title'][0]['_text'].strip()\n\n    #if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration\n        #video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()\n\n    if '_text' in dictified['video'][0]['size'][0]:  #size exists for 1 piece\n        video_dict['size'] = sum([int(i['size'][0]['_text']) for i in dictified['video']])\n\n    if '_text' in dictified['video'][0]['file'][0]:  #link exist\n        video_dict['links'] = [i['file'][0]['_text'].strip() for i in dictified['video']]\n\n    if '_text' in dictified['flashvars'][0]:\n        video_dict['flashvars'] = dictified['flashvars'][0]['_text'].strip()\n\n    return video_dict", "code_tokens": ["def", "ckplayer_get_info_by_xml", "(", "ckinfo", ")", ":", "e", "=", "ET", ".", "XML", "(", "ckinfo", ")", "video_dict", "=", "{", "'title'", ":", "''", ",", "#'duration': 0,", "'links'", ":", "[", "]", ",", "'size'", ":", "0", ",", "'flashvars'", ":", "''", ",", "}", "dictified", "=", "dictify", "(", "e", ")", "[", "'ckplayer'", "]", "if", "'info'", "in", "dictified", ":", "if", "'_text'", "in", "dictified", "[", "'info'", "]", "[", "0", "]", "[", "'title'", "]", "[", "0", "]", ":", "#title", "video_dict", "[", "'title'", "]", "=", "dictified", "[", "'info'", "]", "[", "0", "]", "[", "'title'", "]", "[", "0", "]", "[", "'_text'", "]", ".", "strip", "(", ")", "#if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip():  #duration", "#video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip()", "if", "'_text'", "in", "dictified", "[", "'video'", "]", "[", "0", "]", "[", "'size'", "]", "[", "0", "]", ":", "#size exists for 1 piece", "video_dict", "[", "'size'", "]", "=", "sum", "(", "[", "int", "(", "i", "[", "'size'", "]", "[", "0", "]", "[", "'_text'", "]", ")", "for", "i", "in", "dictified", "[", "'video'", "]", "]", ")", "if", "'_text'", "in", "dictified", "[", "'video'", "]", "[", "0", "]", "[", "'file'", "]", "[", "0", "]", ":", "#link exist", "video_dict", "[", "'links'", "]", "=", "[", "i", "[", "'file'", "]", "[", "0", "]", "[", "'_text'", "]", ".", "strip", "(", ")", "for", "i", "in", "dictified", "[", "'video'", "]", "]", "if", "'_text'", "in", "dictified", "[", "'flashvars'", "]", "[", "0", "]", ":", "video_dict", "[", "'flashvars'", "]", "=", "dictified", "[", "'flashvars'", "]", "[", "0", "]", "[", "'_text'", "]", ".", "strip", "(", ")", "return", "video_dict"], "docstring": "str->dict\n    Information for CKPlayer API content.", "docstring_tokens": ["str", "-", ">", "dict", "Information", "for", "CKPlayer", "API", "content", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/ckplayer.py#L13-L39"}, "target": "str - > dict Information for CKPlayer API content .", "arguments": {"gen_args_0": {"arg_0": "def ckplayer_get_info_by_xml ( ckinfo ) : e = ET . XML ( ckinfo ) video_dict = { 'title' : '' , #'duration': 0, 'links' : [ ] , 'size' : 0 , 'flashvars' : '' , } dictified = dictify ( e ) [ 'ckplayer' ] if 'info' in dictified : if '_text' in dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] : #title video_dict [ 'title' ] = dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] [ '_text' ] . strip ( ) #if dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip(): #duration #video_dict['title'] = dictify(e)['ckplayer']['info'][0]['title'][0]['_text'].strip() if '_text' in dictified [ 'video' ] [ 0 ] [ 'size' ] [ 0 ] : #size exists for 1 piece video_dict [ 'size' ] = sum ( [ int ( i [ 'size' ] [ 0 ] [ '_text' ] ) for i in dictified [ 'video' ] ] ) if '_text' in dictified [ 'video' ] [ 0 ] [ 'file' ] [ 0 ] : #link exist video_dict [ 'links' ] = [ i [ 'file' ] [ 0 ] [ '_text' ] . strip ( ) for i in dictified [ 'video' ] ] if '_text' in dictified [ 'flashvars' ] [ 0 ] : video_dict [ 'flashvars' ] = dictified [ 'flashvars' ] [ 0 ] [ '_text' ] . strip ( ) return video_dict", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [[""]], "filtered_resps": [""], "doc_hash": "c4319b8537245d3fae834e29a96855de4717446ede4c73a4040794b4e2f0998c", "prompt_hash": "2aff4e800316163b35c179a332ca6c744b98d7debbb35571403e9fe4eabcdbf5", "target_hash": "4f3f1b58a5090f78167bc883dd9f6292c43d0ec2dca6aa352ae04528444f4581", "smoothed_bleu_4": 0.004539992976248485}
{"doc_id": 10, "doc": {"id": 10, "repo": "soimort/you-get", "path": "src/you_get/extractors/ixigua.py", "func_name": "get_video_url_from_video_id", "original_string": "def get_video_url_from_video_id(video_id):\n    \"\"\"Splicing URLs according to video ID to get video details\"\"\"\n    # from js\n    data = [\"\"] * 256\n    for index, _ in enumerate(data):\n        t = index\n        for i in range(8):\n            t = -306674912 ^ unsigned_right_shitf(t, 1) if 1 & t else unsigned_right_shitf(t, 1)\n        data[index] = t\n\n    def tmp():\n        rand_num = random.random()\n        path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\".format(video_id=video_id,\n                                                                              random_num=str(rand_num)[2:])\n        e = o = r = -1\n        i, a = 0, len(path)\n        while i < a:\n            e = ord(path[i])\n            i += 1\n            if e < 128:\n                r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ e)]\n            else:\n                if e < 2048:\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (192 | e >> 6 & 31))]\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n                else:\n                    if 55296 <= e < 57344:\n                        e = (1023 & e) + 64\n                        i += 1\n                        o = 1023 & t.url(i)\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (240 | e >> 8 & 7))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 2 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | o >> 6 & 15 | (3 & e) << 4))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & o))]\n                    else:\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (224 | e >> 12 & 15))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 6 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n\n        return \"https://ib.365yg.com{path}&s={param}\".format(path=path, param=unsigned_right_shitf(r ^ -1, 0))\n\n    while 1:\n        url = tmp()\n        if url.split(\"=\")[-1][0] != \"-\":  # 参数s不能为负数\n            return url", "language": "python", "code": "def get_video_url_from_video_id(video_id):\n    \"\"\"Splicing URLs according to video ID to get video details\"\"\"\n    # from js\n    data = [\"\"] * 256\n    for index, _ in enumerate(data):\n        t = index\n        for i in range(8):\n            t = -306674912 ^ unsigned_right_shitf(t, 1) if 1 & t else unsigned_right_shitf(t, 1)\n        data[index] = t\n\n    def tmp():\n        rand_num = random.random()\n        path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\".format(video_id=video_id,\n                                                                              random_num=str(rand_num)[2:])\n        e = o = r = -1\n        i, a = 0, len(path)\n        while i < a:\n            e = ord(path[i])\n            i += 1\n            if e < 128:\n                r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ e)]\n            else:\n                if e < 2048:\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (192 | e >> 6 & 31))]\n                    r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n                else:\n                    if 55296 <= e < 57344:\n                        e = (1023 & e) + 64\n                        i += 1\n                        o = 1023 & t.url(i)\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (240 | e >> 8 & 7))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 2 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | o >> 6 & 15 | (3 & e) << 4))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & o))]\n                    else:\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (224 | e >> 12 & 15))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | e >> 6 & 63))]\n                        r = unsigned_right_shitf(r, 8) ^ data[255 & (r ^ (128 | 63 & e))]\n\n        return \"https://ib.365yg.com{path}&s={param}\".format(path=path, param=unsigned_right_shitf(r ^ -1, 0))\n\n    while 1:\n        url = tmp()\n        if url.split(\"=\")[-1][0] != \"-\":  # 参数s不能为负数\n            return url", "code_tokens": ["def", "get_video_url_from_video_id", "(", "video_id", ")", ":", "# from js", "data", "=", "[", "\"\"", "]", "*", "256", "for", "index", ",", "_", "in", "enumerate", "(", "data", ")", ":", "t", "=", "index", "for", "i", "in", "range", "(", "8", ")", ":", "t", "=", "-", "306674912", "^", "unsigned_right_shitf", "(", "t", ",", "1", ")", "if", "1", "&", "t", "else", "unsigned_right_shitf", "(", "t", ",", "1", ")", "data", "[", "index", "]", "=", "t", "def", "tmp", "(", ")", ":", "rand_num", "=", "random", ".", "random", "(", ")", "path", "=", "\"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\"", ".", "format", "(", "video_id", "=", "video_id", ",", "random_num", "=", "str", "(", "rand_num", ")", "[", "2", ":", "]", ")", "e", "=", "o", "=", "r", "=", "-", "1", "i", ",", "a", "=", "0", ",", "len", "(", "path", ")", "while", "i", "<", "a", ":", "e", "=", "ord", "(", "path", "[", "i", "]", ")", "i", "+=", "1", "if", "e", "<", "128", ":", "r", "=", "unsigned_right_shitf", "(", "r", ",", "8", ")", "^", "data", "[", "255", "&", "(", "r", "^", "e", ")", "]", "else", ":", "if", "e", "<", "2048", ":", "r", "=", "unsigned_right_shitf", "(", "r", ",", "8", ")", "^", "data", "[", "255", "&", "(", "r", "^", "(", "192", "|", "e", ">>", "6", "&", "31", ")", ")", "]", "r", "=", "unsigned_right_shitf", "(", "r", ",", "8", ")", "^", "data", "[", "255", "&", "(", "r", "^", "(", "128", "|", "63", "&", "e", ")", ")", "]", "else", ":", "if", "55296", "<=", "e", "<", "57344", ":", "e", "=", "(", "1023", "&", "e", ")", "+", "64", "i", "+=", "1", "o", "=", "1023", "&", "t", ".", "url", "(", "i", ")", "r", "=", "unsigned_right_shitf", "(", "r", ",", "8", ")", "^", "data", "[", "255", "&", "(", "r", "^", "(", "240", "|", "e", ">>", "8", "&", "7", ")", ")", "]", "r", "=", "unsigned_right_shitf", "(", "r", ",", "8", ")", "^", "data", "[", "255", "&", "(", "r", "^", "(", "128", "|", "e", ">>", "2", "&", "63", ")", ")", "]", "r", "=", "unsigned_right_shitf", "(", "r", ",", "8", ")", "^", "data", "[", "255", "&", "(", "r", "^", "(", "128", "|", "o", ">>", "6", "&", "15", "|", "(", "3", "&", "e", ")", "<<", "4", ")", ")", "]", "r", "=", "unsigned_right_shitf", "(", "r", ",", "8", ")", "^", "data", "[", "255", "&", "(", "r", "^", "(", "128", "|", "63", "&", "o", ")", ")", "]", "else", ":", "r", "=", "unsigned_right_shitf", "(", "r", ",", "8", ")", "^", "data", "[", "255", "&", "(", "r", "^", "(", "224", "|", "e", ">>", "12", "&", "15", ")", ")", "]", "r", "=", "unsigned_right_shitf", "(", "r", ",", "8", ")", "^", "data", "[", "255", "&", "(", "r", "^", "(", "128", "|", "e", ">>", "6", "&", "63", ")", ")", "]", "r", "=", "unsigned_right_shitf", "(", "r", ",", "8", ")", "^", "data", "[", "255", "&", "(", "r", "^", "(", "128", "|", "63", "&", "e", ")", ")", "]", "return", "\"https://ib.365yg.com{path}&s={param}\"", ".", "format", "(", "path", "=", "path", ",", "param", "=", "unsigned_right_shitf", "(", "r", "^", "-", "1", ",", "0", ")", ")", "while", "1", ":", "url", "=", "tmp", "(", ")", "if", "url", ".", "split", "(", "\"=\"", ")", "[", "-", "1", "]", "[", "0", "]", "!=", "\"-\"", ":", "# 参数s不能为负数", "return", "url"], "docstring": "Splicing URLs according to video ID to get video details", "docstring_tokens": ["Splicing", "URLs", "according", "to", "video", "ID", "to", "get", "video", "details"], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/ixigua.py#L34-L78"}, "target": "Splicing URLs according to video ID to get video details", "arguments": {"gen_args_0": {"arg_0": "def get_video_url_from_video_id ( video_id ) : # from js data = [ \"\" ] * 256 for index , _ in enumerate ( data ) : t = index for i in range ( 8 ) : t = - 306674912 ^ unsigned_right_shitf ( t , 1 ) if 1 & t else unsigned_right_shitf ( t , 1 ) data [ index ] = t def tmp ( ) : rand_num = random . random ( ) path = \"/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}\" . format ( video_id = video_id , random_num = str ( rand_num ) [ 2 : ] ) e = o = r = - 1 i , a = 0 , len ( path ) while i < a : e = ord ( path [ i ] ) i += 1 if e < 128 : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ e ) ] else : if e < 2048 : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 192 | e >> 6 & 31 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] else : if 55296 <= e < 57344 : e = ( 1023 & e ) + 64 i += 1 o = 1023 & t . url ( i ) r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 240 | e >> 8 & 7 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 2 & 63 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | o >> 6 & 15 | ( 3 & e ) << 4 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & o ) ) ] else : r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 224 | e >> 12 & 15 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 6 & 63 ) ) ] r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] return \"https://ib.365yg.com{path}&s={param}\" . format ( path = path , param = unsigned_right_shitf ( r ^ - 1 , 0 ) ) while 1 : url = tmp ( ) if url . split ( \"=\" ) [ - 1 ] [ 0 ] != \"-\" : # 参数s不能为负数 return url", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["else : continue"]], "filtered_resps": ["else : continue"], "doc_hash": "f348a26c03b8e31564aa110ec6c82a3cf368d9224fd2b85c08667cedba356e14", "prompt_hash": "8eaa0ad329385e87a9d5aa8f061c42e56b0e4b99972aa6619fe5a2746fe83419", "target_hash": "5fac0e92c9cd6d8eff76ed7f2f5a177f03980b5927c94bc4fa35223485bfcb48", "smoothed_bleu_4": 1.030392379836391e-76}
{"doc_id": 11, "doc": {"id": 11, "repo": "soimort/you-get", "path": "src/you_get/extractors/mgtv.py", "func_name": "MGTV.get_mgtv_real_url", "original_string": "def get_mgtv_real_url(url):\n        \"\"\"str->list of str\n        Give you the real URLs.\"\"\"\n        content = loads(get_content(url))\n        m3u_url = content['info']\n        split = urlsplit(m3u_url)\n        \n        base_url = \"{scheme}://{netloc}{path}/\".format(scheme = split[0],\n                                                      netloc = split[1],\n                                                      path = dirname(split[2]))\n\n        content = get_content(content['info'])  #get the REAL M3U url, maybe to be changed later?\n        segment_list = []\n        segments_size = 0\n        for i in content.split():\n            if not i.startswith('#'):  #not the best way, better we use the m3u8 package\n                segment_list.append(base_url + i)\n            # use ext-info for fast size calculate\n            elif i.startswith('#EXT-MGTV-File-SIZE:'):\n                segments_size += int(i[i.rfind(':')+1:])\n\n        return m3u_url, segments_size, segment_list", "language": "python", "code": "def get_mgtv_real_url(url):\n        \"\"\"str->list of str\n        Give you the real URLs.\"\"\"\n        content = loads(get_content(url))\n        m3u_url = content['info']\n        split = urlsplit(m3u_url)\n        \n        base_url = \"{scheme}://{netloc}{path}/\".format(scheme = split[0],\n                                                      netloc = split[1],\n                                                      path = dirname(split[2]))\n\n        content = get_content(content['info'])  #get the REAL M3U url, maybe to be changed later?\n        segment_list = []\n        segments_size = 0\n        for i in content.split():\n            if not i.startswith('#'):  #not the best way, better we use the m3u8 package\n                segment_list.append(base_url + i)\n            # use ext-info for fast size calculate\n            elif i.startswith('#EXT-MGTV-File-SIZE:'):\n                segments_size += int(i[i.rfind(':')+1:])\n\n        return m3u_url, segments_size, segment_list", "code_tokens": ["def", "get_mgtv_real_url", "(", "url", ")", ":", "content", "=", "loads", "(", "get_content", "(", "url", ")", ")", "m3u_url", "=", "content", "[", "'info'", "]", "split", "=", "urlsplit", "(", "m3u_url", ")", "base_url", "=", "\"{scheme}://{netloc}{path}/\"", ".", "format", "(", "scheme", "=", "split", "[", "0", "]", ",", "netloc", "=", "split", "[", "1", "]", ",", "path", "=", "dirname", "(", "split", "[", "2", "]", ")", ")", "content", "=", "get_content", "(", "content", "[", "'info'", "]", ")", "#get the REAL M3U url, maybe to be changed later?", "segment_list", "=", "[", "]", "segments_size", "=", "0", "for", "i", "in", "content", ".", "split", "(", ")", ":", "if", "not", "i", ".", "startswith", "(", "'#'", ")", ":", "#not the best way, better we use the m3u8 package", "segment_list", ".", "append", "(", "base_url", "+", "i", ")", "# use ext-info for fast size calculate", "elif", "i", ".", "startswith", "(", "'#EXT-MGTV-File-SIZE:'", ")", ":", "segments_size", "+=", "int", "(", "i", "[", "i", ".", "rfind", "(", "':'", ")", "+", "1", ":", "]", ")", "return", "m3u_url", ",", "segments_size", ",", "segment_list"], "docstring": "str->list of str\n        Give you the real URLs.", "docstring_tokens": ["str", "-", ">", "list", "of", "str", "Give", "you", "the", "real", "URLs", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/mgtv.py#L37-L58"}, "target": "str - > list of str Give you the real URLs .", "arguments": {"gen_args_0": {"arg_0": "def get_mgtv_real_url ( url ) : content = loads ( get_content ( url ) ) m3u_url = content [ 'info' ] split = urlsplit ( m3u_url ) base_url = \"{scheme}://{netloc}{path}/\" . format ( scheme = split [ 0 ] , netloc = split [ 1 ] , path = dirname ( split [ 2 ] ) ) content = get_content ( content [ 'info' ] ) #get the REAL M3U url, maybe to be changed later? segment_list = [ ] segments_size = 0 for i in content . split ( ) : if not i . startswith ( '#' ) : #not the best way, better we use the m3u8 package segment_list . append ( base_url + i ) # use ext-info for fast size calculate elif i . startswith ( '#EXT-MGTV-File-SIZE:' ) : segments_size += int ( i [ i . rfind ( ':' ) + 1 : ] ) return m3u_url , segments_size , segment_list", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [[""]], "filtered_resps": [""], "doc_hash": "729378b7b5c638ca60d890399678a1867cb98931cea964484fbaacc05a2aaaeb", "prompt_hash": "310cc32538dc03a3ca4bcd752bcea7b703f0e5fa2d7f09e94369545117ab4afe", "target_hash": "718c8bea6e97010879cf1721d8d857edae1d6bc6daea7a8a0e379593ed370603", "smoothed_bleu_4": 0.000614421235332821}
{"doc_id": 12, "doc": {"id": 12, "repo": "soimort/you-get", "path": "src/you_get/util/fs.py", "func_name": "legitimize", "original_string": "def legitimize(text, os=detect_os()):\n    \"\"\"Converts a string to a valid filename.\n    \"\"\"\n\n    # POSIX systems\n    text = text.translate({\n        0: None,\n        ord('/'): '-',\n        ord('|'): '-',\n    })\n\n    # FIXME: do some filesystem detection\n    if os == 'windows' or os == 'cygwin' or os == 'wsl':\n        # Windows (non-POSIX namespace)\n        text = text.translate({\n            # Reserved in Windows VFAT and NTFS\n            ord(':'): '-',\n            ord('*'): '-',\n            ord('?'): '-',\n            ord('\\\\'): '-',\n            ord('\\\"'): '\\'',\n            # Reserved in Windows VFAT\n            ord('+'): '-',\n            ord('<'): '-',\n            ord('>'): '-',\n            ord('['): '(',\n            ord(']'): ')',\n            ord('\\t'): ' ',\n        })\n    else:\n        # *nix\n        if os == 'mac':\n            # Mac OS HFS+\n            text = text.translate({\n                ord(':'): '-',\n            })\n\n        # Remove leading .\n        if text.startswith(\".\"):\n            text = text[1:]\n\n    text = text[:80] # Trim to 82 Unicode characters long\n    return text", "language": "python", "code": "def legitimize(text, os=detect_os()):\n    \"\"\"Converts a string to a valid filename.\n    \"\"\"\n\n    # POSIX systems\n    text = text.translate({\n        0: None,\n        ord('/'): '-',\n        ord('|'): '-',\n    })\n\n    # FIXME: do some filesystem detection\n    if os == 'windows' or os == 'cygwin' or os == 'wsl':\n        # Windows (non-POSIX namespace)\n        text = text.translate({\n            # Reserved in Windows VFAT and NTFS\n            ord(':'): '-',\n            ord('*'): '-',\n            ord('?'): '-',\n            ord('\\\\'): '-',\n            ord('\\\"'): '\\'',\n            # Reserved in Windows VFAT\n            ord('+'): '-',\n            ord('<'): '-',\n            ord('>'): '-',\n            ord('['): '(',\n            ord(']'): ')',\n            ord('\\t'): ' ',\n        })\n    else:\n        # *nix\n        if os == 'mac':\n            # Mac OS HFS+\n            text = text.translate({\n                ord(':'): '-',\n            })\n\n        # Remove leading .\n        if text.startswith(\".\"):\n            text = text[1:]\n\n    text = text[:80] # Trim to 82 Unicode characters long\n    return text", "code_tokens": ["def", "legitimize", "(", "text", ",", "os", "=", "detect_os", "(", ")", ")", ":", "# POSIX systems", "text", "=", "text", ".", "translate", "(", "{", "0", ":", "None", ",", "ord", "(", "'/'", ")", ":", "'-'", ",", "ord", "(", "'|'", ")", ":", "'-'", ",", "}", ")", "# FIXME: do some filesystem detection", "if", "os", "==", "'windows'", "or", "os", "==", "'cygwin'", "or", "os", "==", "'wsl'", ":", "# Windows (non-POSIX namespace)", "text", "=", "text", ".", "translate", "(", "{", "# Reserved in Windows VFAT and NTFS", "ord", "(", "':'", ")", ":", "'-'", ",", "ord", "(", "'*'", ")", ":", "'-'", ",", "ord", "(", "'?'", ")", ":", "'-'", ",", "ord", "(", "'\\\\'", ")", ":", "'-'", ",", "ord", "(", "'\\\"'", ")", ":", "'\\''", ",", "# Reserved in Windows VFAT", "ord", "(", "'+'", ")", ":", "'-'", ",", "ord", "(", "'<'", ")", ":", "'-'", ",", "ord", "(", "'>'", ")", ":", "'-'", ",", "ord", "(", "'['", ")", ":", "'('", ",", "ord", "(", "']'", ")", ":", "')'", ",", "ord", "(", "'\\t'", ")", ":", "' '", ",", "}", ")", "else", ":", "# *nix", "if", "os", "==", "'mac'", ":", "# Mac OS HFS+", "text", "=", "text", ".", "translate", "(", "{", "ord", "(", "':'", ")", ":", "'-'", ",", "}", ")", "# Remove leading .", "if", "text", ".", "startswith", "(", "\".\"", ")", ":", "text", "=", "text", "[", "1", ":", "]", "text", "=", "text", "[", ":", "80", "]", "# Trim to 82 Unicode characters long", "return", "text"], "docstring": "Converts a string to a valid filename.", "docstring_tokens": ["Converts", "a", "string", "to", "a", "valid", "filename", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/util/fs.py#L5-L47"}, "target": "Converts a string to a valid filename .", "arguments": {"gen_args_0": {"arg_0": "def legitimize ( text , os = detect_os ( ) ) : # POSIX systems text = text . translate ( { 0 : None , ord ( '/' ) : '-' , ord ( '|' ) : '-' , } ) # FIXME: do some filesystem detection if os == 'windows' or os == 'cygwin' or os == 'wsl' : # Windows (non-POSIX namespace) text = text . translate ( { # Reserved in Windows VFAT and NTFS ord ( ':' ) : '-' , ord ( '*' ) : '-' , ord ( '?' ) : '-' , ord ( '\\\\' ) : '-' , ord ( '\\\"' ) : '\\'' , # Reserved in Windows VFAT ord ( '+' ) : '-' , ord ( '<' ) : '-' , ord ( '>' ) : '-' , ord ( '[' ) : '(' , ord ( ']' ) : ')' , ord ( '\\t' ) : ' ' , } ) else : # *nix if os == 'mac' : # Mac OS HFS+ text = text . translate ( { ord ( ':' ) : '-' , } ) # Remove leading . if text . startswith ( \".\" ) : text = text [ 1 : ] text = text [ : 80 ] # Trim to 82 Unicode characters long return text", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [[""]], "filtered_resps": [""], "doc_hash": "6828190b96ae087466352e816c2d72dd3378d5353516ac72ac776bbf8f35b0a0", "prompt_hash": "63bb22112cfa58c3d15c96415665c7900206b6673da5760168937da846d62da7", "target_hash": "61356d4277172aabfc02defa5a307f398405767da1398e8bf9a636a5796aa14b", "smoothed_bleu_4": 0.033546262790251184}
{"doc_id": 13, "doc": {"id": 13, "repo": "soimort/you-get", "path": "src/you_get/extractors/cbs.py", "func_name": "cbs_download", "original_string": "def cbs_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads CBS videos by URL.\n    \"\"\"\n\n    html = get_content(url)\n    pid = match1(html, r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'')\n    title = match1(html, r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"')\n\n    theplatform_download_by_pid(pid, title, output_dir=output_dir, merge=merge, info_only=info_only)", "language": "python", "code": "def cbs_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"Downloads CBS videos by URL.\n    \"\"\"\n\n    html = get_content(url)\n    pid = match1(html, r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'')\n    title = match1(html, r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"')\n\n    theplatform_download_by_pid(pid, title, output_dir=output_dir, merge=merge, info_only=info_only)", "code_tokens": ["def", "cbs_download", "(", "url", ",", "output_dir", "=", "'.'", ",", "merge", "=", "True", ",", "info_only", "=", "False", ",", "*", "*", "kwargs", ")", ":", "html", "=", "get_content", "(", "url", ")", "pid", "=", "match1", "(", "html", ",", "r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\''", ")", "title", "=", "match1", "(", "html", ",", "r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"'", ")", "theplatform_download_by_pid", "(", "pid", ",", "title", ",", "output_dir", "=", "output_dir", ",", "merge", "=", "merge", ",", "info_only", "=", "info_only", ")"], "docstring": "Downloads CBS videos by URL.", "docstring_tokens": ["Downloads", "CBS", "videos", "by", "URL", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/cbs.py#L9-L17"}, "target": "Downloads CBS videos by URL .", "arguments": {"gen_args_0": {"arg_0": "def cbs_download ( url , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : html = get_content ( url ) pid = match1 ( html , r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'' ) title = match1 ( html , r'video\\.settings\\.title\\s*=\\s*\\\"([^\\\"]+)\\\"' ) theplatform_download_by_pid ( pid , title , output_dir = output_dir , merge = merge , info_only = info_only )", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def cbs_search ( query , * * kwargs ) : return"]], "filtered_resps": ["def cbs_search ( query , * * kwargs ) : return"], "doc_hash": "01fb7833bcda3f03bed132e2882e5e5dfcfbc49a163870e0cbacfbc7faf261fc", "prompt_hash": "d0ed868408d4b8c1cf3fc57649e0816c22098b91ad3d50a3145dfe3835872b2a", "target_hash": "4c3d0244e81d8d695a29ef5c3809e4c0e12fd585dc34ce34be7d7d0766318d97", "smoothed_bleu_4": 8.182477900877295}
{"doc_id": 14, "doc": {"id": 14, "repo": "soimort/you-get", "path": "src/you_get/extractors/iqiyi.py", "func_name": "Iqiyi.download", "original_string": "def download(self, **kwargs):\n        \"\"\"Override the original one\n        Ugly ugly dirty hack\"\"\"\n        if 'json_output' in kwargs and kwargs['json_output']:\n            json_output.output(self)\n        elif 'info_only' in kwargs and kwargs['info_only']:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Display the stream\n                stream_id = kwargs['stream_id']\n                if 'index' not in kwargs:\n                    self.p(stream_id)\n                else:\n                    self.p_i(stream_id)\n            else:\n                # Display all available streams\n                if 'index' not in kwargs:\n                    self.p([])\n                else:\n                    stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n                    self.p_i(stream_id)\n\n        else:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Download the stream\n                stream_id = kwargs['stream_id']\n            else:\n                # Download stream with the best quality\n                stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n\n            if 'index' not in kwargs:\n                self.p(stream_id)\n            else:\n                self.p_i(stream_id)\n\n            if stream_id in self.streams:\n                urls = self.streams[stream_id]['src']\n                ext = self.streams[stream_id]['container']\n                total_size = self.streams[stream_id]['size']\n            else:\n                urls = self.dash_streams[stream_id]['src']\n                ext = self.dash_streams[stream_id]['container']\n                total_size = self.dash_streams[stream_id]['size']\n\n            if not urls:\n                log.wtf('[Failed] Cannot extract video source.')\n            # For legacy main()\n            \n            #Here's the change!!\n            download_url_ffmpeg(urls[0], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)\n\n            if not kwargs['caption']:\n                print('Skipping captions.')\n                return\n            for lang in self.caption_tracks:\n                filename = '%s.%s.srt' % (get_filename(self.title), lang)\n                print('Saving %s ... ' % filename, end=\"\", flush=True)\n                srt = self.caption_tracks[lang]\n                with open(os.path.join(kwargs['output_dir'], filename),\n                          'w', encoding='utf-8') as x:\n                    x.write(srt)\n                print('Done.')", "language": "python", "code": "def download(self, **kwargs):\n        \"\"\"Override the original one\n        Ugly ugly dirty hack\"\"\"\n        if 'json_output' in kwargs and kwargs['json_output']:\n            json_output.output(self)\n        elif 'info_only' in kwargs and kwargs['info_only']:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Display the stream\n                stream_id = kwargs['stream_id']\n                if 'index' not in kwargs:\n                    self.p(stream_id)\n                else:\n                    self.p_i(stream_id)\n            else:\n                # Display all available streams\n                if 'index' not in kwargs:\n                    self.p([])\n                else:\n                    stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n                    self.p_i(stream_id)\n\n        else:\n            if 'stream_id' in kwargs and kwargs['stream_id']:\n                # Download the stream\n                stream_id = kwargs['stream_id']\n            else:\n                # Download stream with the best quality\n                stream_id = self.streams_sorted[0]['id'] if 'id' in self.streams_sorted[0] else self.streams_sorted[0]['itag']\n\n            if 'index' not in kwargs:\n                self.p(stream_id)\n            else:\n                self.p_i(stream_id)\n\n            if stream_id in self.streams:\n                urls = self.streams[stream_id]['src']\n                ext = self.streams[stream_id]['container']\n                total_size = self.streams[stream_id]['size']\n            else:\n                urls = self.dash_streams[stream_id]['src']\n                ext = self.dash_streams[stream_id]['container']\n                total_size = self.dash_streams[stream_id]['size']\n\n            if not urls:\n                log.wtf('[Failed] Cannot extract video source.')\n            # For legacy main()\n            \n            #Here's the change!!\n            download_url_ffmpeg(urls[0], self.title, 'mp4', output_dir=kwargs['output_dir'], merge=kwargs['merge'], stream=False)\n\n            if not kwargs['caption']:\n                print('Skipping captions.')\n                return\n            for lang in self.caption_tracks:\n                filename = '%s.%s.srt' % (get_filename(self.title), lang)\n                print('Saving %s ... ' % filename, end=\"\", flush=True)\n                srt = self.caption_tracks[lang]\n                with open(os.path.join(kwargs['output_dir'], filename),\n                          'w', encoding='utf-8') as x:\n                    x.write(srt)\n                print('Done.')", "code_tokens": ["def", "download", "(", "self", ",", "*", "*", "kwargs", ")", ":", "if", "'json_output'", "in", "kwargs", "and", "kwargs", "[", "'json_output'", "]", ":", "json_output", ".", "output", "(", "self", ")", "elif", "'info_only'", "in", "kwargs", "and", "kwargs", "[", "'info_only'", "]", ":", "if", "'stream_id'", "in", "kwargs", "and", "kwargs", "[", "'stream_id'", "]", ":", "# Display the stream", "stream_id", "=", "kwargs", "[", "'stream_id'", "]", "if", "'index'", "not", "in", "kwargs", ":", "self", ".", "p", "(", "stream_id", ")", "else", ":", "self", ".", "p_i", "(", "stream_id", ")", "else", ":", "# Display all available streams", "if", "'index'", "not", "in", "kwargs", ":", "self", ".", "p", "(", "[", "]", ")", "else", ":", "stream_id", "=", "self", ".", "streams_sorted", "[", "0", "]", "[", "'id'", "]", "if", "'id'", "in", "self", ".", "streams_sorted", "[", "0", "]", "else", "self", ".", "streams_sorted", "[", "0", "]", "[", "'itag'", "]", "self", ".", "p_i", "(", "stream_id", ")", "else", ":", "if", "'stream_id'", "in", "kwargs", "and", "kwargs", "[", "'stream_id'", "]", ":", "# Download the stream", "stream_id", "=", "kwargs", "[", "'stream_id'", "]", "else", ":", "# Download stream with the best quality", "stream_id", "=", "self", ".", "streams_sorted", "[", "0", "]", "[", "'id'", "]", "if", "'id'", "in", "self", ".", "streams_sorted", "[", "0", "]", "else", "self", ".", "streams_sorted", "[", "0", "]", "[", "'itag'", "]", "if", "'index'", "not", "in", "kwargs", ":", "self", ".", "p", "(", "stream_id", ")", "else", ":", "self", ".", "p_i", "(", "stream_id", ")", "if", "stream_id", "in", "self", ".", "streams", ":", "urls", "=", "self", ".", "streams", "[", "stream_id", "]", "[", "'src'", "]", "ext", "=", "self", ".", "streams", "[", "stream_id", "]", "[", "'container'", "]", "total_size", "=", "self", ".", "streams", "[", "stream_id", "]", "[", "'size'", "]", "else", ":", "urls", "=", "self", ".", "dash_streams", "[", "stream_id", "]", "[", "'src'", "]", "ext", "=", "self", ".", "dash_streams", "[", "stream_id", "]", "[", "'container'", "]", "total_size", "=", "self", ".", "dash_streams", "[", "stream_id", "]", "[", "'size'", "]", "if", "not", "urls", ":", "log", ".", "wtf", "(", "'[Failed] Cannot extract video source.'", ")", "# For legacy main()", "#Here's the change!!", "download_url_ffmpeg", "(", "urls", "[", "0", "]", ",", "self", ".", "title", ",", "'mp4'", ",", "output_dir", "=", "kwargs", "[", "'output_dir'", "]", ",", "merge", "=", "kwargs", "[", "'merge'", "]", ",", "stream", "=", "False", ")", "if", "not", "kwargs", "[", "'caption'", "]", ":", "print", "(", "'Skipping captions.'", ")", "return", "for", "lang", "in", "self", ".", "caption_tracks", ":", "filename", "=", "'%s.%s.srt'", "%", "(", "get_filename", "(", "self", ".", "title", ")", ",", "lang", ")", "print", "(", "'Saving %s ... '", "%", "filename", ",", "end", "=", "\"\"", ",", "flush", "=", "True", ")", "srt", "=", "self", ".", "caption_tracks", "[", "lang", "]", "with", "open", "(", "os", ".", "path", ".", "join", "(", "kwargs", "[", "'output_dir'", "]", ",", "filename", ")", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "x", ":", "x", ".", "write", "(", "srt", ")", "print", "(", "'Done.'", ")"], "docstring": "Override the original one\n        Ugly ugly dirty hack", "docstring_tokens": ["Override", "the", "original", "one", "Ugly", "ugly", "dirty", "hack"], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/iqiyi.py#L158-L218"}, "target": "Override the original one Ugly ugly dirty hack", "arguments": {"gen_args_0": {"arg_0": "def download ( self , * * kwargs ) : if 'json_output' in kwargs and kwargs [ 'json_output' ] : json_output . output ( self ) elif 'info_only' in kwargs and kwargs [ 'info_only' ] : if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : # Display the stream stream_id = kwargs [ 'stream_id' ] if 'index' not in kwargs : self . p ( stream_id ) else : self . p_i ( stream_id ) else : # Display all available streams if 'index' not in kwargs : self . p ( [ ] ) else : stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] self . p_i ( stream_id ) else : if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : # Download the stream stream_id = kwargs [ 'stream_id' ] else : # Download stream with the best quality stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] if 'index' not in kwargs : self . p ( stream_id ) else : self . p_i ( stream_id ) if stream_id in self . streams : urls = self . streams [ stream_id ] [ 'src' ] ext = self . streams [ stream_id ] [ 'container' ] total_size = self . streams [ stream_id ] [ 'size' ] else : urls = self . dash_streams [ stream_id ] [ 'src' ] ext = self . dash_streams [ stream_id ] [ 'container' ] total_size = self . dash_streams [ stream_id ] [ 'size' ] if not urls : log . wtf ( '[Failed] Cannot extract video source.' ) # For legacy main() #Here's the change!! download_url_ffmpeg ( urls [ 0 ] , self . title , 'mp4' , output_dir = kwargs [ 'output_dir' ] , merge = kwargs [ 'merge' ] , stream = False ) if not kwargs [ 'caption' ] : print ( 'Skipping captions.' ) return for lang in self . caption_tracks : filename = '%s.%s.srt' % ( get_filename ( self . title ) , lang ) print ( 'Saving %s ... ' % filename , end = \"\" , flush = True ) srt = self . caption_tracks [ lang ] with open ( os . path . join ( kwargs [ 'output_dir' ] , filename ) , 'w' , encoding = 'utf-8' ) as x : x . write ( srt ) print ( 'Done.' )", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["else : print ( 'Skipping captions.' ) return if not kwargs ["]], "filtered_resps": ["else : print ( 'Skipping captions.' ) return if not kwargs ["], "doc_hash": "a60ac80052470dd559f4f1df0d82a3227b97e34e40344ce547fc59d241fe3e3b", "prompt_hash": "6d685aa14919be3c17af78df5f315dab2e1583e1544cb060030485f61253c393", "target_hash": "236f7a43c0762a7800c02a23719f26d4fb1250af7c120b3180725f7679ce6995", "smoothed_bleu_4": 8.585639149516246e-77}
{"doc_id": 15, "doc": {"id": 15, "repo": "soimort/you-get", "path": "src/you_get/extractors/acfun.py", "func_name": "acfun_download_by_vid", "original_string": "def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"str, str, str, bool, bool ->None\n\n    Download Acfun video by vid.\n\n    Call Acfun API, decide which site to use, and pass the job to its\n    extractor.\n    \"\"\"\n\n    #first call the main parasing API\n    info = json.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid))\n\n    sourceType = info['sourceType']\n\n    #decide sourceId to know which extractor to use\n    if 'sourceId' in info: sourceId = info['sourceId']\n    # danmakuId = info['danmakuId']\n\n    #call extractor decided by sourceId\n    if sourceType == 'sina':\n        sina_download_by_vid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'youku':\n        youku_download_by_vid(sourceId, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n    elif sourceType == 'tudou':\n        tudou_download_by_iid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'qq':\n        qq_download_by_vid(sourceId, title, True, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'letv':\n        letvcloud_download_by_vu(sourceId, '2d8c027396', title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'zhuzhan':\n        #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this\n#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player\n#old code removed\n        url = 'http://www.acfun.cn/v/ac' + vid\n        yk_streams = youku_acfun_proxy(info['sourceId'], info['encode'], url)\n        seq = ['mp4hd3', 'mp4hd2', 'mp4hd', 'flvhd']\n        for t in seq:\n            if yk_streams.get(t):\n                preferred = yk_streams[t]\n                break\n#total_size in the json could be incorrect(F.I. 0)\n        size = 0\n        for url in preferred[0]:\n            _, _, seg_size = url_info(url)\n            size += seg_size\n#fallback to flvhd is not quite possible\n        if re.search(r'fid=[0-9A-Z\\-]*.flv', preferred[0][0]):\n            ext = 'flv'\n        else:\n            ext = 'mp4'\n        print_info(site_info, title, ext, size)\n        if not info_only:\n            download_urls(preferred[0], title, ext, size, output_dir=output_dir, merge=merge)\n    else:\n        raise NotImplementedError(sourceType)\n\n    if not info_only and not dry_run:\n        if not kwargs['caption']:\n            print('Skipping danmaku.')\n            return\n        try:\n            title = get_filename(title)\n            print('Downloading %s ...\\n' % (title + '.cmt.json'))\n            cmt = get_srt_json(vid)\n            with open(os.path.join(output_dir, title + '.cmt.json'), 'w', encoding='utf-8') as x:\n                x.write(cmt)\n        except:\n            pass", "language": "python", "code": "def acfun_download_by_vid(vid, title, output_dir='.', merge=True, info_only=False, **kwargs):\n    \"\"\"str, str, str, bool, bool ->None\n\n    Download Acfun video by vid.\n\n    Call Acfun API, decide which site to use, and pass the job to its\n    extractor.\n    \"\"\"\n\n    #first call the main parasing API\n    info = json.loads(get_content('http://www.acfun.cn/video/getVideo.aspx?id=' + vid))\n\n    sourceType = info['sourceType']\n\n    #decide sourceId to know which extractor to use\n    if 'sourceId' in info: sourceId = info['sourceId']\n    # danmakuId = info['danmakuId']\n\n    #call extractor decided by sourceId\n    if sourceType == 'sina':\n        sina_download_by_vid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'youku':\n        youku_download_by_vid(sourceId, title=title, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n    elif sourceType == 'tudou':\n        tudou_download_by_iid(sourceId, title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'qq':\n        qq_download_by_vid(sourceId, title, True, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'letv':\n        letvcloud_download_by_vu(sourceId, '2d8c027396', title, output_dir=output_dir, merge=merge, info_only=info_only)\n    elif sourceType == 'zhuzhan':\n        #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this\n#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player\n#old code removed\n        url = 'http://www.acfun.cn/v/ac' + vid\n        yk_streams = youku_acfun_proxy(info['sourceId'], info['encode'], url)\n        seq = ['mp4hd3', 'mp4hd2', 'mp4hd', 'flvhd']\n        for t in seq:\n            if yk_streams.get(t):\n                preferred = yk_streams[t]\n                break\n#total_size in the json could be incorrect(F.I. 0)\n        size = 0\n        for url in preferred[0]:\n            _, _, seg_size = url_info(url)\n            size += seg_size\n#fallback to flvhd is not quite possible\n        if re.search(r'fid=[0-9A-Z\\-]*.flv', preferred[0][0]):\n            ext = 'flv'\n        else:\n            ext = 'mp4'\n        print_info(site_info, title, ext, size)\n        if not info_only:\n            download_urls(preferred[0], title, ext, size, output_dir=output_dir, merge=merge)\n    else:\n        raise NotImplementedError(sourceType)\n\n    if not info_only and not dry_run:\n        if not kwargs['caption']:\n            print('Skipping danmaku.')\n            return\n        try:\n            title = get_filename(title)\n            print('Downloading %s ...\\n' % (title + '.cmt.json'))\n            cmt = get_srt_json(vid)\n            with open(os.path.join(output_dir, title + '.cmt.json'), 'w', encoding='utf-8') as x:\n                x.write(cmt)\n        except:\n            pass", "code_tokens": ["def", "acfun_download_by_vid", "(", "vid", ",", "title", ",", "output_dir", "=", "'.'", ",", "merge", "=", "True", ",", "info_only", "=", "False", ",", "*", "*", "kwargs", ")", ":", "#first call the main parasing API", "info", "=", "json", ".", "loads", "(", "get_content", "(", "'http://www.acfun.cn/video/getVideo.aspx?id='", "+", "vid", ")", ")", "sourceType", "=", "info", "[", "'sourceType'", "]", "#decide sourceId to know which extractor to use", "if", "'sourceId'", "in", "info", ":", "sourceId", "=", "info", "[", "'sourceId'", "]", "# danmakuId = info['danmakuId']", "#call extractor decided by sourceId", "if", "sourceType", "==", "'sina'", ":", "sina_download_by_vid", "(", "sourceId", ",", "title", ",", "output_dir", "=", "output_dir", ",", "merge", "=", "merge", ",", "info_only", "=", "info_only", ")", "elif", "sourceType", "==", "'youku'", ":", "youku_download_by_vid", "(", "sourceId", ",", "title", "=", "title", ",", "output_dir", "=", "output_dir", ",", "merge", "=", "merge", ",", "info_only", "=", "info_only", ",", "*", "*", "kwargs", ")", "elif", "sourceType", "==", "'tudou'", ":", "tudou_download_by_iid", "(", "sourceId", ",", "title", ",", "output_dir", "=", "output_dir", ",", "merge", "=", "merge", ",", "info_only", "=", "info_only", ")", "elif", "sourceType", "==", "'qq'", ":", "qq_download_by_vid", "(", "sourceId", ",", "title", ",", "True", ",", "output_dir", "=", "output_dir", ",", "merge", "=", "merge", ",", "info_only", "=", "info_only", ")", "elif", "sourceType", "==", "'letv'", ":", "letvcloud_download_by_vu", "(", "sourceId", ",", "'2d8c027396'", ",", "title", ",", "output_dir", "=", "output_dir", ",", "merge", "=", "merge", ",", "info_only", "=", "info_only", ")", "elif", "sourceType", "==", "'zhuzhan'", ":", "#As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this", "#In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player", "#old code removed", "url", "=", "'http://www.acfun.cn/v/ac'", "+", "vid", "yk_streams", "=", "youku_acfun_proxy", "(", "info", "[", "'sourceId'", "]", ",", "info", "[", "'encode'", "]", ",", "url", ")", "seq", "=", "[", "'mp4hd3'", ",", "'mp4hd2'", ",", "'mp4hd'", ",", "'flvhd'", "]", "for", "t", "in", "seq", ":", "if", "yk_streams", ".", "get", "(", "t", ")", ":", "preferred", "=", "yk_streams", "[", "t", "]", "break", "#total_size in the json could be incorrect(F.I. 0)", "size", "=", "0", "for", "url", "in", "preferred", "[", "0", "]", ":", "_", ",", "_", ",", "seg_size", "=", "url_info", "(", "url", ")", "size", "+=", "seg_size", "#fallback to flvhd is not quite possible", "if", "re", ".", "search", "(", "r'fid=[0-9A-Z\\-]*.flv'", ",", "preferred", "[", "0", "]", "[", "0", "]", ")", ":", "ext", "=", "'flv'", "else", ":", "ext", "=", "'mp4'", "print_info", "(", "site_info", ",", "title", ",", "ext", ",", "size", ")", "if", "not", "info_only", ":", "download_urls", "(", "preferred", "[", "0", "]", ",", "title", ",", "ext", ",", "size", ",", "output_dir", "=", "output_dir", ",", "merge", "=", "merge", ")", "else", ":", "raise", "NotImplementedError", "(", "sourceType", ")", "if", "not", "info_only", "and", "not", "dry_run", ":", "if", "not", "kwargs", "[", "'caption'", "]", ":", "print", "(", "'Skipping danmaku.'", ")", "return", "try", ":", "title", "=", "get_filename", "(", "title", ")", "print", "(", "'Downloading %s ...\\n'", "%", "(", "title", "+", "'.cmt.json'", ")", ")", "cmt", "=", "get_srt_json", "(", "vid", ")", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "title", "+", "'.cmt.json'", ")", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "x", ":", "x", ".", "write", "(", "cmt", ")", "except", ":", "pass"], "docstring": "str, str, str, bool, bool ->None\n\n    Download Acfun video by vid.\n\n    Call Acfun API, decide which site to use, and pass the job to its\n    extractor.", "docstring_tokens": ["str", "str", "str", "bool", "bool", "-", ">", "None"], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/extractors/acfun.py#L42-L109"}, "target": "str str str bool bool - > None", "arguments": {"gen_args_0": {"arg_0": "def acfun_download_by_vid ( vid , title , output_dir = '.' , merge = True , info_only = False , * * kwargs ) : #first call the main parasing API info = json . loads ( get_content ( 'http://www.acfun.cn/video/getVideo.aspx?id=' + vid ) ) sourceType = info [ 'sourceType' ] #decide sourceId to know which extractor to use if 'sourceId' in info : sourceId = info [ 'sourceId' ] # danmakuId = info['danmakuId'] #call extractor decided by sourceId if sourceType == 'sina' : sina_download_by_vid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'youku' : youku_download_by_vid ( sourceId , title = title , output_dir = output_dir , merge = merge , info_only = info_only , * * kwargs ) elif sourceType == 'tudou' : tudou_download_by_iid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'qq' : qq_download_by_vid ( sourceId , title , True , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'letv' : letvcloud_download_by_vu ( sourceId , '2d8c027396' , title , output_dir = output_dir , merge = merge , info_only = info_only ) elif sourceType == 'zhuzhan' : #As in Jul.28.2016, Acfun is using embsig to anti hotlink so we need to pass this #In Mar. 2017 there is a dedicated ``acfun_proxy'' in youku cloud player #old code removed url = 'http://www.acfun.cn/v/ac' + vid yk_streams = youku_acfun_proxy ( info [ 'sourceId' ] , info [ 'encode' ] , url ) seq = [ 'mp4hd3' , 'mp4hd2' , 'mp4hd' , 'flvhd' ] for t in seq : if yk_streams . get ( t ) : preferred = yk_streams [ t ] break #total_size in the json could be incorrect(F.I. 0) size = 0 for url in preferred [ 0 ] : _ , _ , seg_size = url_info ( url ) size += seg_size #fallback to flvhd is not quite possible if re . search ( r'fid=[0-9A-Z\\-]*.flv' , preferred [ 0 ] [ 0 ] ) : ext = 'flv' else : ext = 'mp4' print_info ( site_info , title , ext , size ) if not info_only : download_urls ( preferred [ 0 ] , title , ext , size , output_dir = output_dir , merge = merge ) else : raise NotImplementedError ( sourceType ) if not info_only and not dry_run : if not kwargs [ 'caption' ] : print ( 'Skipping danmaku.' ) return try : title = get_filename ( title ) print ( 'Downloading %s ...\\n' % ( title + '.cmt.json' ) ) cmt = get_srt_json ( vid ) with open ( os . path . join ( output_dir , title + '.cmt.json' ) , 'w' , encoding = 'utf-8' ) as x : x . write ( cmt ) except : pass", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [[""]], "filtered_resps": [""], "doc_hash": "7929f5cac4e1f7f353f2e71bf782b65be2246ca5e123e6daef13ee81f0777c39", "prompt_hash": "38a59af4e2e51b622ffd7f093086075468402ed3946c79b7d4e836f06729dffc", "target_hash": "adab47829686fbe946af9c6475d32b26963d8c8cde948e460df8bfce5f2f89c9", "smoothed_bleu_4": 0.033546262790251184}
{"doc_id": 16, "doc": {"id": 16, "repo": "soimort/you-get", "path": "src/you_get/common.py", "func_name": "matchall", "original_string": "def matchall(text, patterns):\n    \"\"\"Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.\n    \"\"\"\n\n    ret = []\n    for pattern in patterns:\n        match = re.findall(pattern, text)\n        ret += match\n\n    return ret", "language": "python", "code": "def matchall(text, patterns):\n    \"\"\"Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.\n    \"\"\"\n\n    ret = []\n    for pattern in patterns:\n        match = re.findall(pattern, text)\n        ret += match\n\n    return ret", "code_tokens": ["def", "matchall", "(", "text", ",", "patterns", ")", ":", "ret", "=", "[", "]", "for", "pattern", "in", "patterns", ":", "match", "=", "re", ".", "findall", "(", "pattern", ",", "text", ")", "ret", "+=", "match", "return", "ret"], "docstring": "Scans through a string for substrings matched some patterns.\n\n    Args:\n        text: A string to be scanned.\n        patterns: a list of regex pattern.\n\n    Returns:\n        a list if matched. empty if not.", "docstring_tokens": ["Scans", "through", "a", "string", "for", "substrings", "matched", "some", "patterns", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L252-L268"}, "target": "Scans through a string for substrings matched some patterns .", "arguments": {"gen_args_0": {"arg_0": "def matchall ( text , patterns ) : ret = [ ] for pattern in patterns : match = re . findall ( pattern , text ) ret += match return ret", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def main () :\n\n    text = input ()"]], "filtered_resps": ["def main () :\n\n    text = input ()"], "doc_hash": "907120fc0329c8e4138096d5e359276f9d47a479dd830938815cca618f80dec1", "prompt_hash": "8d29e8c467e611c3b12a370d8ecd034f4240651c9c663b0b2d79042e09acc912", "target_hash": "41c9c379e8339f94e1353c98eb9f0d8be4e9d453eebdf38573db90b7ead68c45", "smoothed_bleu_4": 1.3258762269321836e-76}
{"doc_id": 17, "doc": {"id": 17, "repo": "soimort/you-get", "path": "src/you_get/common.py", "func_name": "parse_query_param", "original_string": "def parse_query_param(url, param):\n    \"\"\"Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.\n    \"\"\"\n\n    try:\n        return parse.parse_qs(parse.urlparse(url).query)[param][0]\n    except:\n        return None", "language": "python", "code": "def parse_query_param(url, param):\n    \"\"\"Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.\n    \"\"\"\n\n    try:\n        return parse.parse_qs(parse.urlparse(url).query)[param][0]\n    except:\n        return None", "code_tokens": ["def", "parse_query_param", "(", "url", ",", "param", ")", ":", "try", ":", "return", "parse", ".", "parse_qs", "(", "parse", ".", "urlparse", "(", "url", ")", ".", "query", ")", "[", "param", "]", "[", "0", "]", "except", ":", "return", "None"], "docstring": "Parses the query string of a URL and returns the value of a parameter.\n\n    Args:\n        url: A URL.\n        param: A string representing the name of the parameter.\n\n    Returns:\n        The value of the parameter.", "docstring_tokens": ["Parses", "the", "query", "string", "of", "a", "URL", "and", "returns", "the", "value", "of", "a", "parameter", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L285-L299"}, "target": "Parses the query string of a URL and returns the value of a parameter .", "arguments": {"gen_args_0": {"arg_0": "def parse_query_param ( url , param ) : try : return parse . parse_qs ( parse . urlparse ( url ) . query ) [ param ] [ 0 ] except : return None", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def get_query_param ( url , param ) : if not"]], "filtered_resps": ["def get_query_param ( url , param ) : if not"], "doc_hash": "e3ca585144caaff3ec916457c2640eff14ff962e7c70f2042ed0ed2f56475b45", "prompt_hash": "48f3bb41bcea2510ba2215f10b67c276da192885a53671c6ee76a5ed7aa62b13", "target_hash": "62e5710d9992317aabb72b1246e0ec09631c9c13aaee551d2cbda457e356f7de", "smoothed_bleu_4": 8.4131642172441}
{"doc_id": 18, "doc": {"id": 18, "repo": "soimort/you-get", "path": "src/you_get/common.py", "func_name": "get_content", "original_string": "def get_content(url, headers={}, decoded=True):\n    \"\"\"Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n\n    logging.debug('get_content: %s' % url)\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n\n    response = urlopen_with_retry(req)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type', ''), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset, 'ignore')\n        else:\n            data = data.decode('utf-8', 'ignore')\n\n    return data", "language": "python", "code": "def get_content(url, headers={}, decoded=True):\n    \"\"\"Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n\n    logging.debug('get_content: %s' % url)\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n\n    response = urlopen_with_retry(req)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type', ''), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset, 'ignore')\n        else:\n            data = data.decode('utf-8', 'ignore')\n\n    return data", "code_tokens": ["def", "get_content", "(", "url", ",", "headers", "=", "{", "}", ",", "decoded", "=", "True", ")", ":", "logging", ".", "debug", "(", "'get_content: %s'", "%", "url", ")", "req", "=", "request", ".", "Request", "(", "url", ",", "headers", "=", "headers", ")", "if", "cookies", ":", "cookies", ".", "add_cookie_header", "(", "req", ")", "req", ".", "headers", ".", "update", "(", "req", ".", "unredirected_hdrs", ")", "response", "=", "urlopen_with_retry", "(", "req", ")", "data", "=", "response", ".", "read", "(", ")", "# Handle HTTP compression for gzip and deflate (zlib)", "content_encoding", "=", "response", ".", "getheader", "(", "'Content-Encoding'", ")", "if", "content_encoding", "==", "'gzip'", ":", "data", "=", "ungzip", "(", "data", ")", "elif", "content_encoding", "==", "'deflate'", ":", "data", "=", "undeflate", "(", "data", ")", "# Decode the response body", "if", "decoded", ":", "charset", "=", "match1", "(", "response", ".", "getheader", "(", "'Content-Type'", ",", "''", ")", ",", "r'charset=([\\w-]+)'", ")", "if", "charset", "is", "not", "None", ":", "data", "=", "data", ".", "decode", "(", "charset", ",", "'ignore'", ")", "else", ":", "data", "=", "data", ".", "decode", "(", "'utf-8'", ",", "'ignore'", ")", "return", "data"], "docstring": "Gets the content of a URL via sending a HTTP GET request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.", "docstring_tokens": ["Gets", "the", "content", "of", "a", "URL", "via", "sending", "a", "HTTP", "GET", "request", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L415-L454"}, "target": "Gets the content of a URL via sending a HTTP GET request .", "arguments": {"gen_args_0": {"arg_0": "def get_content ( url , headers = { } , decoded = True ) : logging . debug ( 'get_content: %s' % url ) req = request . Request ( url , headers = headers ) if cookies : cookies . add_cookie_header ( req ) req . headers . update ( req . unredirected_hdrs ) response = urlopen_with_retry ( req ) data = response . read ( ) # Handle HTTP compression for gzip and deflate (zlib) content_encoding = response . getheader ( 'Content-Encoding' ) if content_encoding == 'gzip' : data = ungzip ( data ) elif content_encoding == 'deflate' : data = undeflate ( data ) # Decode the response body if decoded : charset = match1 ( response . getheader ( 'Content-Type' , '' ) , r'charset=([\\w-]+)' ) if charset is not None : data = data . decode ( charset , 'ignore' ) else : data = data . decode ( 'utf-8' , 'ignore' ) return data", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [[""]], "filtered_resps": [""], "doc_hash": "b9b16bee681fb52763b27d692a7207279305165fed9a34b663a6ac2503cc64a4", "prompt_hash": "8fe3bbeed11916f48d3fec8d1faad6beec7a44216642cc6fda4435fa53fa0c51", "target_hash": "639d085da8e4bf0bf4474005953fb3258c909c384006e0a6a3e77462951e9051", "smoothed_bleu_4": 0.00022603294069810544}
{"doc_id": 19, "doc": {"id": 19, "repo": "soimort/you-get", "path": "src/you_get/common.py", "func_name": "post_content", "original_string": "def post_content(url, headers={}, post_data={}, decoded=True, **kwargs):\n    \"\"\"Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n    if kwargs.get('post_data_raw'):\n        logging.debug('post_content: %s\\npost_data_raw: %s' % (url, kwargs['post_data_raw']))\n    else:\n        logging.debug('post_content: %s\\npost_data: %s' % (url, post_data))\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n    if kwargs.get('post_data_raw'):\n        post_data_enc = bytes(kwargs['post_data_raw'], 'utf-8')\n    else:\n        post_data_enc = bytes(parse.urlencode(post_data), 'utf-8')\n    response = urlopen_with_retry(req, data=post_data_enc)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type'), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset)\n        else:\n            data = data.decode('utf-8')\n\n    return data", "language": "python", "code": "def post_content(url, headers={}, post_data={}, decoded=True, **kwargs):\n    \"\"\"Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.\n    \"\"\"\n    if kwargs.get('post_data_raw'):\n        logging.debug('post_content: %s\\npost_data_raw: %s' % (url, kwargs['post_data_raw']))\n    else:\n        logging.debug('post_content: %s\\npost_data: %s' % (url, post_data))\n\n    req = request.Request(url, headers=headers)\n    if cookies:\n        cookies.add_cookie_header(req)\n        req.headers.update(req.unredirected_hdrs)\n    if kwargs.get('post_data_raw'):\n        post_data_enc = bytes(kwargs['post_data_raw'], 'utf-8')\n    else:\n        post_data_enc = bytes(parse.urlencode(post_data), 'utf-8')\n    response = urlopen_with_retry(req, data=post_data_enc)\n    data = response.read()\n\n    # Handle HTTP compression for gzip and deflate (zlib)\n    content_encoding = response.getheader('Content-Encoding')\n    if content_encoding == 'gzip':\n        data = ungzip(data)\n    elif content_encoding == 'deflate':\n        data = undeflate(data)\n\n    # Decode the response body\n    if decoded:\n        charset = match1(\n            response.getheader('Content-Type'), r'charset=([\\w-]+)'\n        )\n        if charset is not None:\n            data = data.decode(charset)\n        else:\n            data = data.decode('utf-8')\n\n    return data", "code_tokens": ["def", "post_content", "(", "url", ",", "headers", "=", "{", "}", ",", "post_data", "=", "{", "}", ",", "decoded", "=", "True", ",", "*", "*", "kwargs", ")", ":", "if", "kwargs", ".", "get", "(", "'post_data_raw'", ")", ":", "logging", ".", "debug", "(", "'post_content: %s\\npost_data_raw: %s'", "%", "(", "url", ",", "kwargs", "[", "'post_data_raw'", "]", ")", ")", "else", ":", "logging", ".", "debug", "(", "'post_content: %s\\npost_data: %s'", "%", "(", "url", ",", "post_data", ")", ")", "req", "=", "request", ".", "Request", "(", "url", ",", "headers", "=", "headers", ")", "if", "cookies", ":", "cookies", ".", "add_cookie_header", "(", "req", ")", "req", ".", "headers", ".", "update", "(", "req", ".", "unredirected_hdrs", ")", "if", "kwargs", ".", "get", "(", "'post_data_raw'", ")", ":", "post_data_enc", "=", "bytes", "(", "kwargs", "[", "'post_data_raw'", "]", ",", "'utf-8'", ")", "else", ":", "post_data_enc", "=", "bytes", "(", "parse", ".", "urlencode", "(", "post_data", ")", ",", "'utf-8'", ")", "response", "=", "urlopen_with_retry", "(", "req", ",", "data", "=", "post_data_enc", ")", "data", "=", "response", ".", "read", "(", ")", "# Handle HTTP compression for gzip and deflate (zlib)", "content_encoding", "=", "response", ".", "getheader", "(", "'Content-Encoding'", ")", "if", "content_encoding", "==", "'gzip'", ":", "data", "=", "ungzip", "(", "data", ")", "elif", "content_encoding", "==", "'deflate'", ":", "data", "=", "undeflate", "(", "data", ")", "# Decode the response body", "if", "decoded", ":", "charset", "=", "match1", "(", "response", ".", "getheader", "(", "'Content-Type'", ")", ",", "r'charset=([\\w-]+)'", ")", "if", "charset", "is", "not", "None", ":", "data", "=", "data", ".", "decode", "(", "charset", ")", "else", ":", "data", "=", "data", ".", "decode", "(", "'utf-8'", ")", "return", "data"], "docstring": "Post the content of a URL via sending a HTTP POST request.\n\n    Args:\n        url: A URL.\n        headers: Request headers used by the client.\n        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n\n    Returns:\n        The content as a string.", "docstring_tokens": ["Post", "the", "content", "of", "a", "URL", "via", "sending", "a", "HTTP", "POST", "request", "."], "sha": "b746ac01c9f39de94cac2d56f665285b0523b974", "url": "https://github.com/soimort/you-get/blob/b746ac01c9f39de94cac2d56f665285b0523b974/src/you_get/common.py#L457-L501"}, "target": "Post the content of a URL via sending a HTTP POST request .", "arguments": {"gen_args_0": {"arg_0": "def post_content ( url , headers = { } , post_data = { } , decoded = True , * * kwargs ) : if kwargs . get ( 'post_data_raw' ) : logging . debug ( 'post_content: %s\\npost_data_raw: %s' % ( url , kwargs [ 'post_data_raw' ] ) ) else : logging . debug ( 'post_content: %s\\npost_data: %s' % ( url , post_data ) ) req = request . Request ( url , headers = headers ) if cookies : cookies . add_cookie_header ( req ) req . headers . update ( req . unredirected_hdrs ) if kwargs . get ( 'post_data_raw' ) : post_data_enc = bytes ( kwargs [ 'post_data_raw' ] , 'utf-8' ) else : post_data_enc = bytes ( parse . urlencode ( post_data ) , 'utf-8' ) response = urlopen_with_retry ( req , data = post_data_enc ) data = response . read ( ) # Handle HTTP compression for gzip and deflate (zlib) content_encoding = response . getheader ( 'Content-Encoding' ) if content_encoding == 'gzip' : data = ungzip ( data ) elif content_encoding == 'deflate' : data = undeflate ( data ) # Decode the response body if decoded : charset = match1 ( response . getheader ( 'Content-Type' ) , r'charset=([\\w-]+)' ) if charset is not None : data = data . decode ( charset ) else : data = data . decode ( 'utf-8' ) return data", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [[""]], "filtered_resps": [""], "doc_hash": "80b144af0ebc393678cf665996f8c002cea8b1d6c640ce3fcc232e1fa7c96802", "prompt_hash": "650b22094a9752333741ff1a4568c5fab75f4bb923348883711af70831bcf7ff", "target_hash": "f529fc8933264e0351ba06ebeb9b3afe62952a4e1b8082df12329153b553dedb", "smoothed_bleu_4": 0.00022603294069810544}
