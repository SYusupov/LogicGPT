{"doc_id": 0, "doc": {"id": 0, "repo": "Shopify/kubernetes-deploy", "path": "lib/kubernetes-deploy/deferred_summary_logging.rb", "func_name": "KubernetesDeploy.DeferredSummaryLogging.print_summary", "original_string": "def print_summary(status)\n      status_string = status.to_s.humanize.upcase\n      if status == :success\n        heading(\"Result: \", status_string, :green)\n        level = :info\n      elsif status == :timed_out\n        heading(\"Result: \", status_string, :yellow)\n        level = :fatal\n      else\n        heading(\"Result: \", status_string, :red)\n        level = :fatal\n      end\n\n      if (actions_sentence = summary.actions_sentence.presence)\n        public_send(level, actions_sentence)\n        blank_line(level)\n      end\n\n      summary.paragraphs.each do |para|\n        msg_lines = para.split(\"\\n\")\n        msg_lines.each { |line| public_send(level, line) }\n        blank_line(level) unless para == summary.paragraphs.last\n      end\n    end", "language": "ruby", "code": "def print_summary(status)\n      status_string = status.to_s.humanize.upcase\n      if status == :success\n        heading(\"Result: \", status_string, :green)\n        level = :info\n      elsif status == :timed_out\n        heading(\"Result: \", status_string, :yellow)\n        level = :fatal\n      else\n        heading(\"Result: \", status_string, :red)\n        level = :fatal\n      end\n\n      if (actions_sentence = summary.actions_sentence.presence)\n        public_send(level, actions_sentence)\n        blank_line(level)\n      end\n\n      summary.paragraphs.each do |para|\n        msg_lines = para.split(\"\\n\")\n        msg_lines.each { |line| public_send(level, line) }\n        blank_line(level) unless para == summary.paragraphs.last\n      end\n    end", "code_tokens": ["def", "print_summary", "(", "status", ")", "status_string", "=", "status", ".", "to_s", ".", "humanize", ".", "upcase", "if", "status", "==", ":success", "heading", "(", "\"Result: \"", ",", "status_string", ",", ":green", ")", "level", "=", ":info", "elsif", "status", "==", ":timed_out", "heading", "(", "\"Result: \"", ",", "status_string", ",", ":yellow", ")", "level", "=", ":fatal", "else", "heading", "(", "\"Result: \"", ",", "status_string", ",", ":red", ")", "level", "=", ":fatal", "end", "if", "(", "actions_sentence", "=", "summary", ".", "actions_sentence", ".", "presence", ")", "public_send", "(", "level", ",", "actions_sentence", ")", "blank_line", "(", "level", ")", "end", "summary", ".", "paragraphs", ".", "each", "do", "|", "para", "|", "msg_lines", "=", "para", ".", "split", "(", "\"\\n\"", ")", "msg_lines", ".", "each", "{", "|", "line", "|", "public_send", "(", "level", ",", "line", ")", "}", "blank_line", "(", "level", ")", "unless", "para", "==", "summary", ".", "paragraphs", ".", "last", "end", "end"], "docstring": "Outputs the deferred summary information saved via @logger.summary.add_action and @logger.summary.add_paragraph", "docstring_tokens": ["Outputs", "the", "deferred", "summary", "information", "saved", "via"], "sha": "ab599ee9a4c1268d5a538a622a83160223fd916c", "url": "https://github.com/Shopify/kubernetes-deploy/blob/ab599ee9a4c1268d5a538a622a83160223fd916c/lib/kubernetes-deploy/deferred_summary_logging.rb#L37-L60"}, "target": "Outputs the deferred summary information saved via", "arguments": {"gen_args_0": {"arg_0": "def print_summary ( status ) status_string = status . to_s . humanize . upcase if status == :success heading ( \"Result: \" , status_string , :green ) level = :info elsif status == :timed_out heading ( \"Result: \" , status_string , :yellow ) level = :fatal else heading ( \"Result: \" , status_string , :red ) level = :fatal end if ( actions_sentence = summary . actions_sentence . presence ) public_send ( level , actions_sentence ) blank_line ( level ) end summary . paragraphs . each do | para | msg_lines = para . split ( \"\\n\" ) msg_lines . each { | line | public_send ( level , line ) } blank_line ( level ) unless para == summary . paragraphs . last end end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def heading ( *args , color = :info ) args . shift if args"]], "filtered_resps": ["def heading ( *args , color = :info ) args . shift if args"], "doc_hash": "2bebe9881a1902c060dd7b17dba862729baf53c7814b003b6e0cda1f97f7dfa3", "prompt_hash": "ef62c7cc0a5efedb7ae229dcfe7567d732b8282a8af3e44a73890a8872abca42", "target_hash": "77a8e6af304af998ef998556187300116661704889163f2e330467f8df91e939", "smoothed_bleu_4": 8.020866590058631e-77}
{"doc_id": 1, "doc": {"id": 1, "repo": "Shopify/kubernetes-deploy", "path": "lib/kubernetes-deploy/deploy_task.rb", "func_name": "KubernetesDeploy.DeployTask.find_bad_files_from_kubectl_output", "original_string": "def find_bad_files_from_kubectl_output(line)\n      # stderr often contains one or more lines like the following, from which we can extract the file path(s):\n      # Error from server (TypeOfError): error when creating \"/path/to/service-gqq5oh.yml\": Service \"web\" is invalid:\n\n      line.scan(%r{\"(/\\S+\\.ya?ml\\S*)\"}).each_with_object([]) do |matches, bad_files|\n        matches.each do |path|\n          content = File.read(path) if File.file?(path)\n          bad_files << { filename: File.basename(path), err: line, content: content }\n        end\n      end\n    end", "language": "ruby", "code": "def find_bad_files_from_kubectl_output(line)\n      # stderr often contains one or more lines like the following, from which we can extract the file path(s):\n      # Error from server (TypeOfError): error when creating \"/path/to/service-gqq5oh.yml\": Service \"web\" is invalid:\n\n      line.scan(%r{\"(/\\S+\\.ya?ml\\S*)\"}).each_with_object([]) do |matches, bad_files|\n        matches.each do |path|\n          content = File.read(path) if File.file?(path)\n          bad_files << { filename: File.basename(path), err: line, content: content }\n        end\n      end\n    end", "code_tokens": ["def", "find_bad_files_from_kubectl_output", "(", "line", ")", "# stderr often contains one or more lines like the following, from which we can extract the file path(s):", "# Error from server (TypeOfError): error when creating \"/path/to/service-gqq5oh.yml\": Service \"web\" is invalid:", "line", ".", "scan", "(", "%r{", "\\S", "\\.", "\\S", "}", ")", ".", "each_with_object", "(", "[", "]", ")", "do", "|", "matches", ",", "bad_files", "|", "matches", ".", "each", "do", "|", "path", "|", "content", "=", "File", ".", "read", "(", "path", ")", "if", "File", ".", "file?", "(", "path", ")", "bad_files", "<<", "{", "filename", ":", "File", ".", "basename", "(", "path", ")", ",", "err", ":", "line", ",", "content", ":", "content", "}", "end", "end", "end"], "docstring": "Inspect the file referenced in the kubectl stderr\n to make it easier for developer to understand what's going on", "docstring_tokens": ["Inspect", "the", "file", "referenced", "in", "the", "kubectl", "stderr", "to", "make", "it", "easier", "for", "developer", "to", "understand", "what", "s", "going", "on"], "sha": "ab599ee9a4c1268d5a538a622a83160223fd916c", "url": "https://github.com/Shopify/kubernetes-deploy/blob/ab599ee9a4c1268d5a538a622a83160223fd916c/lib/kubernetes-deploy/deploy_task.rb#L514-L524"}, "target": "Inspect the file referenced in the kubectl stderr to make it easier for developer to understand what s going on", "arguments": {"gen_args_0": {"arg_0": "def find_bad_files_from_kubectl_output ( line ) # stderr often contains one or more lines like the following, from which we can extract the file path(s): # Error from server (TypeOfError): error when creating \"/path/to/service-gqq5oh.yml\": Service \"web\" is invalid: line . scan ( %r{ \\S \\. \\S } ) . each_with_object ( [ ] ) do | matches , bad_files | matches . each do | path | content = File . read ( path ) if File . file? ( path ) bad_files << { filename : File . basename ( path ) , err : line , content : content } end end end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [[""]], "filtered_resps": [""], "doc_hash": "5822075fd11e5f0b69a26677c2bab0ee63266f4d0fd89215bc154c620bd33915", "prompt_hash": "b8a2f09975c44c8dcad748b24b731ff7446897e416fa5a4303383aed283359ae", "target_hash": "21b751f125253049e0500b836bcf358ca7b55a31b3779c715ef0c13ec1c4e24f", "smoothed_bleu_4": 2.061153622438558e-07}
{"doc_id": 2, "doc": {"id": 2, "repo": "Shopify/kubernetes-deploy", "path": "lib/kubernetes-deploy/deploy_task.rb", "func_name": "KubernetesDeploy.DeployTask.confirm_ejson_keys_not_prunable", "original_string": "def confirm_ejson_keys_not_prunable\n      secret = ejson_provisioner.ejson_keys_secret\n      return unless secret.dig(\"metadata\", \"annotations\", KubernetesResource::LAST_APPLIED_ANNOTATION)\n\n      @logger.error(\"Deploy cannot proceed because protected resource \" \\\n        \"Secret/#{EjsonSecretProvisioner::EJSON_KEYS_SECRET} would be pruned.\")\n      raise EjsonPrunableError\n    rescue Kubectl::ResourceNotFoundError => e\n      @logger.debug(\"Secret/#{EjsonSecretProvisioner::EJSON_KEYS_SECRET} does not exist: #{e}\")\n    end", "language": "ruby", "code": "def confirm_ejson_keys_not_prunable\n      secret = ejson_provisioner.ejson_keys_secret\n      return unless secret.dig(\"metadata\", \"annotations\", KubernetesResource::LAST_APPLIED_ANNOTATION)\n\n      @logger.error(\"Deploy cannot proceed because protected resource \" \\\n        \"Secret/#{EjsonSecretProvisioner::EJSON_KEYS_SECRET} would be pruned.\")\n      raise EjsonPrunableError\n    rescue Kubectl::ResourceNotFoundError => e\n      @logger.debug(\"Secret/#{EjsonSecretProvisioner::EJSON_KEYS_SECRET} does not exist: #{e}\")\n    end", "code_tokens": ["def", "confirm_ejson_keys_not_prunable", "secret", "=", "ejson_provisioner", ".", "ejson_keys_secret", "return", "unless", "secret", ".", "dig", "(", "\"metadata\"", ",", "\"annotations\"", ",", "KubernetesResource", "::", "LAST_APPLIED_ANNOTATION", ")", "@logger", ".", "error", "(", "\"Deploy cannot proceed because protected resource \"", "\"Secret/#{EjsonSecretProvisioner::EJSON_KEYS_SECRET} would be pruned.\"", ")", "raise", "EjsonPrunableError", "rescue", "Kubectl", "::", "ResourceNotFoundError", "=>", "e", "@logger", ".", "debug", "(", "\"Secret/#{EjsonSecretProvisioner::EJSON_KEYS_SECRET} does not exist: #{e}\"", ")", "end"], "docstring": "make sure to never prune the ejson-keys secret", "docstring_tokens": ["make", "sure", "to", "never", "prune", "the", "ejson", "-", "keys", "secret"], "sha": "ab599ee9a4c1268d5a538a622a83160223fd916c", "url": "https://github.com/Shopify/kubernetes-deploy/blob/ab599ee9a4c1268d5a538a622a83160223fd916c/lib/kubernetes-deploy/deploy_task.rb#L570-L579"}, "target": "make sure to never prune the ejson - keys secret", "arguments": {"gen_args_0": {"arg_0": "def confirm_ejson_keys_not_prunable secret = ejson_provisioner . ejson_keys_secret return unless secret . dig ( \"metadata\" , \"annotations\" , KubernetesResource :: LAST_APPLIED_ANNOTATION ) @logger . error ( \"Deploy cannot proceed because protected resource \" \"Secret/#{EjsonSecretProvisioner::EJSON_KEYS_SECRET} would be pruned.\" ) raise EjsonPrunableError rescue Kubectl :: ResourceNotFoundError => e @logger . debug ( \"Secret/#{EjsonSecretProvisioner::EJSON_KEYS_SECRET} does not exist: #{e}\" ) end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [[""]], "filtered_resps": [""], "doc_hash": "18ec33630cc20493a6f6abfe70f57136e3053fc1317b7cc59024b90f1caf242d", "prompt_hash": "71d76de22ea6233cf22ab6378cfdf460cc02cd336f458ca97bf8b683ca899dbe", "target_hash": "ca88b1ce171765157f277b8d1774d849e4119e2702106ae496d60366dc03cb29", "smoothed_bleu_4": 0.004539992976248485}
{"doc_id": 3, "doc": {"id": 3, "repo": "chef/omnibus", "path": "lib/omnibus/compressor.rb", "func_name": "Omnibus.Compressor.for_current_system", "original_string": "def for_current_system(compressors)\n      family = Ohai[\"platform_family\"]\n\n      if family == \"mac_os_x\"\n        if compressors.include?(:dmg)\n          return DMG\n        end\n\n        if compressors.include?(:tgz)\n          return TGZ\n        end\n      end\n\n      if compressors.include?(:tgz)\n        return TGZ\n      else\n        log.info(log_key) { \"No compressor defined for `#{family}'.\" }\n        return Null\n      end\n    end", "language": "ruby", "code": "def for_current_system(compressors)\n      family = Ohai[\"platform_family\"]\n\n      if family == \"mac_os_x\"\n        if compressors.include?(:dmg)\n          return DMG\n        end\n\n        if compressors.include?(:tgz)\n          return TGZ\n        end\n      end\n\n      if compressors.include?(:tgz)\n        return TGZ\n      else\n        log.info(log_key) { \"No compressor defined for `#{family}'.\" }\n        return Null\n      end\n    end", "code_tokens": ["def", "for_current_system", "(", "compressors", ")", "family", "=", "Ohai", "[", "\"platform_family\"", "]", "if", "family", "==", "\"mac_os_x\"", "if", "compressors", ".", "include?", "(", ":dmg", ")", "return", "DMG", "end", "if", "compressors", ".", "include?", "(", ":tgz", ")", "return", "TGZ", "end", "end", "if", "compressors", ".", "include?", "(", ":tgz", ")", "return", "TGZ", "else", "log", ".", "info", "(", "log_key", ")", "{", "\"No compressor defined for `#{family}'.\"", "}", "return", "Null", "end", "end"], "docstring": "Determine the best compressor for the current system. This method returns\n the class, not an instance of the class.\n\n @example\n   Compressor.for_current_system([:dmg, :tgz]) #=> Packager::DMG\n\n @param [Array<Symbol>] compressors\n   the list of configured compressors\n\n @return [~Compressor::Base]", "docstring_tokens": ["Determine", "the", "best", "compressor", "for", "the", "current", "system", ".", "This", "method", "returns", "the", "class", "not", "an", "instance", "of", "the", "class", "."], "sha": "968307c129ee54416f5a4d07ca8f8ca2d2b12825", "url": "https://github.com/chef/omnibus/blob/968307c129ee54416f5a4d07ca8f8ca2d2b12825/lib/omnibus/compressor.rb#L38-L57"}, "target": "Determine the best compressor for the current system . This method returns the class not an instance of the class .", "arguments": {"gen_args_0": {"arg_0": "def for_current_system ( compressors ) family = Ohai [ \"platform_family\" ] if family == \"mac_os_x\" if compressors . include? ( :dmg ) return DMG end if compressors . include? ( :tgz ) return TGZ end end if compressors . include? ( :tgz ) return TGZ else log . info ( log_key ) { \"No compressor defined for `#{family}'.\" } return Null end end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def for_current_system ( compressors ) family = Ohai [ \""]], "filtered_resps": ["def for_current_system ( compressors ) family = Ohai [ \""], "doc_hash": "8a9d428557ea8d2fa7db24fed8c6fded30ebc72bd453168d06cbf1f162d58716", "prompt_hash": "41d7eaf1a75b85ee89bfc1bc53aa7d64a72a76e6f60d51c54bb142904e4d46a3", "target_hash": "a80111cae2986b5c06184aee1fac9cb099c87a9fbd4a44450c16e196e03027f2", "smoothed_bleu_4": 6.241146658461235}
{"doc_id": 4, "doc": {"id": 4, "repo": "chef/omnibus", "path": "lib/omnibus/packagers/bff.rb", "func_name": "Omnibus.Packager::BFF.create_bff_file", "original_string": "def create_bff_file\n      # We are making the assumption that sudo exists.\n      # Unforunately, the owner of the file in the staging directory is what\n      # will be on the target machine, and mkinstallp can't tell you if that\n      # is a bad thing (it usually is).\n      # The match is so we only pick the lowest level of the project dir.\n      # This implies that if we are in /tmp/staging/project/dir/things,\n      # we will chown from 'project' on, rather than 'project/dir', which leaves\n      # project owned by the build user (which is incorrect)\n      # First - let's find out who we are.\n      shellout!(\"sudo chown -Rh 0:0 #{File.join(staging_dir, project.install_dir.match(/^\\/?(\\w+)/).to_s)}\")\n      log.info(log_key) { \"Creating .bff file\" }\n\n      # Since we want the owner to be root, we need to sudo the mkinstallp\n      # command, otherwise it will not have access to the previously chowned\n      # directory.\n      shellout!(\"sudo /usr/sbin/mkinstallp -d #{staging_dir} -T #{File.join(staging_dir, 'gen.template')}\")\n\n      # Print the full contents of the inventory file generated by mkinstallp\n      # from within the staging_dir's .info folder (where control files for the\n      # packaging process are kept.)\n      log.debug(log_key) do\n        \"With .inventory file of:\\n\" + File.read(\"#{File.join( staging_dir, '.info', \"#{safe_base_package_name}.inventory\" )}\")\n      end\n\n      # Copy the resulting package up to the package_dir\n      FileSyncer.glob(File.join(staging_dir, \"tmp/*.bff\")).each do |bff|\n        copy_file(bff, File.join(Config.package_dir, create_bff_file_name))\n      end\n\n    ensure\n      # chown back to original user's uid/gid so cleanup works correctly\n      original_uid = shellout!(\"id -u\").stdout.chomp\n      original_gid = shellout!(\"id -g\").stdout.chomp\n\n      shellout!(\"sudo chown -Rh #{original_uid}:#{original_gid} #{staging_dir}\")\n    end", "language": "ruby", "code": "def create_bff_file\n      # We are making the assumption that sudo exists.\n      # Unforunately, the owner of the file in the staging directory is what\n      # will be on the target machine, and mkinstallp can't tell you if that\n      # is a bad thing (it usually is).\n      # The match is so we only pick the lowest level of the project dir.\n      # This implies that if we are in /tmp/staging/project/dir/things,\n      # we will chown from 'project' on, rather than 'project/dir', which leaves\n      # project owned by the build user (which is incorrect)\n      # First - let's find out who we are.\n      shellout!(\"sudo chown -Rh 0:0 #{File.join(staging_dir, project.install_dir.match(/^\\/?(\\w+)/).to_s)}\")\n      log.info(log_key) { \"Creating .bff file\" }\n\n      # Since we want the owner to be root, we need to sudo the mkinstallp\n      # command, otherwise it will not have access to the previously chowned\n      # directory.\n      shellout!(\"sudo /usr/sbin/mkinstallp -d #{staging_dir} -T #{File.join(staging_dir, 'gen.template')}\")\n\n      # Print the full contents of the inventory file generated by mkinstallp\n      # from within the staging_dir's .info folder (where control files for the\n      # packaging process are kept.)\n      log.debug(log_key) do\n        \"With .inventory file of:\\n\" + File.read(\"#{File.join( staging_dir, '.info', \"#{safe_base_package_name}.inventory\" )}\")\n      end\n\n      # Copy the resulting package up to the package_dir\n      FileSyncer.glob(File.join(staging_dir, \"tmp/*.bff\")).each do |bff|\n        copy_file(bff, File.join(Config.package_dir, create_bff_file_name))\n      end\n\n    ensure\n      # chown back to original user's uid/gid so cleanup works correctly\n      original_uid = shellout!(\"id -u\").stdout.chomp\n      original_gid = shellout!(\"id -g\").stdout.chomp\n\n      shellout!(\"sudo chown -Rh #{original_uid}:#{original_gid} #{staging_dir}\")\n    end", "code_tokens": ["def", "create_bff_file", "# We are making the assumption that sudo exists.", "# Unforunately, the owner of the file in the staging directory is what", "# will be on the target machine, and mkinstallp can't tell you if that", "# is a bad thing (it usually is).", "# The match is so we only pick the lowest level of the project dir.", "# This implies that if we are in /tmp/staging/project/dir/things,", "# we will chown from 'project' on, rather than 'project/dir', which leaves", "# project owned by the build user (which is incorrect)", "# First - let's find out who we are.", "shellout!", "(", "\"sudo chown -Rh 0:0 #{File.join(staging_dir, project.install_dir.match(/^\\/?(\\w+)/).to_s)}\"", ")", "log", ".", "info", "(", "log_key", ")", "{", "\"Creating .bff file\"", "}", "# Since we want the owner to be root, we need to sudo the mkinstallp", "# command, otherwise it will not have access to the previously chowned", "# directory.", "shellout!", "(", "\"sudo /usr/sbin/mkinstallp -d #{staging_dir} -T #{File.join(staging_dir, 'gen.template')}\"", ")", "# Print the full contents of the inventory file generated by mkinstallp", "# from within the staging_dir's .info folder (where control files for the", "# packaging process are kept.)", "log", ".", "debug", "(", "log_key", ")", "do", "\"With .inventory file of:\\n\"", "+", "File", ".", "read", "(", "\"#{File.join( staging_dir, '.info', \"#{safe_base_package_name}.inventory\" )}\"", ")", "end", "# Copy the resulting package up to the package_dir", "FileSyncer", ".", "glob", "(", "File", ".", "join", "(", "staging_dir", ",", "\"tmp/*.bff\"", ")", ")", ".", "each", "do", "|", "bff", "|", "copy_file", "(", "bff", ",", "File", ".", "join", "(", "Config", ".", "package_dir", ",", "create_bff_file_name", ")", ")", "end", "ensure", "# chown back to original user's uid/gid so cleanup works correctly", "original_uid", "=", "shellout!", "(", "\"id -u\"", ")", ".", "stdout", ".", "chomp", "original_gid", "=", "shellout!", "(", "\"id -g\"", ")", ".", "stdout", ".", "chomp", "shellout!", "(", "\"sudo chown -Rh #{original_uid}:#{original_gid} #{staging_dir}\"", ")", "end"], "docstring": "Create the bff file using +mkinstallp+.\n\n Warning: This command runs as sudo! AIX requires the use of sudo to run\n the +mkinstallp+ command.\n\n @return [void]", "docstring_tokens": ["Create", "the", "bff", "file", "using", "+", "mkinstallp", "+", "."], "sha": "968307c129ee54416f5a4d07ca8f8ca2d2b12825", "url": "https://github.com/chef/omnibus/blob/968307c129ee54416f5a4d07ca8f8ca2d2b12825/lib/omnibus/packagers/bff.rb#L207-L243"}, "target": "Create the bff file using + mkinstallp + .", "arguments": {"gen_args_0": {"arg_0": "def create_bff_file # We are making the assumption that sudo exists. # Unforunately, the owner of the file in the staging directory is what # will be on the target machine, and mkinstallp can't tell you if that # is a bad thing (it usually is). # The match is so we only pick the lowest level of the project dir. # This implies that if we are in /tmp/staging/project/dir/things, # we will chown from 'project' on, rather than 'project/dir', which leaves # project owned by the build user (which is incorrect) # First - let's find out who we are. shellout! ( \"sudo chown -Rh 0:0 #{File.join(staging_dir, project.install_dir.match(/^\\/?(\\w+)/).to_s)}\" ) log . info ( log_key ) { \"Creating .bff file\" } # Since we want the owner to be root, we need to sudo the mkinstallp # command, otherwise it will not have access to the previously chowned # directory. shellout! ( \"sudo /usr/sbin/mkinstallp -d #{staging_dir} -T #{File.join(staging_dir, 'gen.template')}\" ) # Print the full contents of the inventory file generated by mkinstallp # from within the staging_dir's .info folder (where control files for the # packaging process are kept.) log . debug ( log_key ) do \"With .inventory file of:\\n\" + File . read ( \"#{File.join( staging_dir, '.info', \"#{safe_base_package_name}.inventory\" )}\" ) end # Copy the resulting package up to the package_dir FileSyncer . glob ( File . join ( staging_dir , \"tmp/*.bff\" ) ) . each do | bff | copy_file ( bff , File . join ( Config . package_dir , create_bff_file_name ) ) end ensure # chown back to original user's uid/gid so cleanup works correctly original_uid = shellout! ( \"id -u\" ) . stdout . chomp original_gid = shellout! ( \"id -g\" ) . stdout . chomp shellout! ( \"sudo chown -Rh #{original_uid}:#{original_gid} #{staging_dir}\" ) end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [[""]], "filtered_resps": [""], "doc_hash": "88dc61c5f15d6447d9bd55b87b1901770e6acd48a30f3a440f64269be8e2147e", "prompt_hash": "5ea7745805cbc30a072a901253cee5e6159be472ab4c3233782ed5916edaa121", "target_hash": "998c5f64b475a06ee93e1b0a7dc05693665ddc1a34447cf41c9126c463b8cd64", "smoothed_bleu_4": 0.012340980408667957}
{"doc_id": 5, "doc": {"id": 5, "repo": "chef/omnibus", "path": "lib/omnibus/file_syncer.rb", "func_name": "Omnibus.FileSyncer.glob", "original_string": "def glob(pattern)\n      pattern = Pathname.new(pattern).cleanpath.to_s\n      Dir.glob(pattern, File::FNM_DOTMATCH).sort.reject do |file|\n        basename = File.basename(file)\n        IGNORED_FILES.include?(basename)\n      end\n    end", "language": "ruby", "code": "def glob(pattern)\n      pattern = Pathname.new(pattern).cleanpath.to_s\n      Dir.glob(pattern, File::FNM_DOTMATCH).sort.reject do |file|\n        basename = File.basename(file)\n        IGNORED_FILES.include?(basename)\n      end\n    end", "code_tokens": ["def", "glob", "(", "pattern", ")", "pattern", "=", "Pathname", ".", "new", "(", "pattern", ")", ".", "cleanpath", ".", "to_s", "Dir", ".", "glob", "(", "pattern", ",", "File", "::", "FNM_DOTMATCH", ")", ".", "sort", ".", "reject", "do", "|", "file", "|", "basename", "=", "File", ".", "basename", "(", "file", ")", "IGNORED_FILES", ".", "include?", "(", "basename", ")", "end", "end"], "docstring": "Glob across the given pattern, accounting for dotfiles, removing Ruby's\n dumb idea to include +'.'+ and +'..'+ as entries.\n\n @param [String] pattern\n   the path or glob pattern to get all files from\n\n @return [Array<String>]\n   the list of all files", "docstring_tokens": ["Glob", "across", "the", "given", "pattern", "accounting", "for", "dotfiles", "removing", "Ruby", "s", "dumb", "idea", "to", "include", "+", ".", "+", "and", "+", "..", "+", "as", "entries", "."], "sha": "968307c129ee54416f5a4d07ca8f8ca2d2b12825", "url": "https://github.com/chef/omnibus/blob/968307c129ee54416f5a4d07ca8f8ca2d2b12825/lib/omnibus/file_syncer.rb#L36-L42"}, "target": "Glob across the given pattern accounting for dotfiles removing Ruby s dumb idea to include + . + and + .. + as entries .", "arguments": {"gen_args_0": {"arg_0": "def glob ( pattern ) pattern = Pathname . new ( pattern ) . cleanpath . to_s Dir . glob ( pattern , File :: FNM_DOTMATCH ) . sort . reject do | file | basename = File . basename ( file ) IGNORED_FILES . include? ( basename ) end end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def IGNORED_FILES = [ 'CVS' , '"]], "filtered_resps": ["def IGNORED_FILES = [ 'CVS' , '"], "doc_hash": "dc00718ca1853669e982ec38725099a0ba15ef8c137046e7d0c837a456b5a480", "prompt_hash": "b50b72e0f8a25b70338fdb7b0a9278c48029be17c70ad671e32a0928ad4f6042", "target_hash": "ba93f3c7e8e32533edd72b52518828acd15eb5bb45005b8f413d5d5a43c8f12b", "smoothed_bleu_4": 3.4253979662232245e-77}
{"doc_id": 6, "doc": {"id": 6, "repo": "chef/omnibus", "path": "lib/omnibus/file_syncer.rb", "func_name": "Omnibus.FileSyncer.sync", "original_string": "def sync(source, destination, options = {})\n      unless File.directory?(source)\n        raise ArgumentError, \"`source' must be a directory, but was a \" \\\n          \"`#{File.ftype(source)}'! If you just want to sync a file, use \" \\\n          \"the `copy' method instead.\"\n      end\n\n      source_files = all_files_under(source, options)\n\n      # Ensure the destination directory exists\n      FileUtils.mkdir_p(destination) unless File.directory?(destination)\n\n      # Copy over the filtered source files\n      source_files.each do |source_file|\n        relative_path = relative_path_for(source_file, source)\n\n        # Create the parent directory\n        parent = File.join(destination, File.dirname(relative_path))\n        FileUtils.mkdir_p(parent) unless File.directory?(parent)\n\n        case File.ftype(source_file).to_sym\n        when :directory\n          FileUtils.mkdir_p(\"#{destination}/#{relative_path}\")\n        when :link\n          target = File.readlink(source_file)\n\n          Dir.chdir(destination) do\n            FileUtils.ln_sf(target, \"#{destination}/#{relative_path}\")\n          end\n        when :file\n          source_stat = File.stat(source_file)\n          # Detect 'files' which are hard links and use ln instead of cp to\n          # duplicate them, provided their source is in place already\n          if hardlink? source_stat\n            if existing = hardlink_sources[[source_stat.dev, source_stat.ino]]\n              FileUtils.ln(existing, \"#{destination}/#{relative_path}\", force: true)\n            else\n              begin\n                FileUtils.cp(source_file, \"#{destination}/#{relative_path}\")\n              rescue Errno::EACCES\n                FileUtils.cp_r(source_file, \"#{destination}/#{relative_path}\", remove_destination: true)\n              end\n              hardlink_sources.store([source_stat.dev, source_stat.ino], \"#{destination}/#{relative_path}\")\n            end\n          else\n            # First attempt a regular copy. If we don't have write\n            # permission on the File, open will probably fail with\n            # EACCES (making it hard to sync files with permission\n            # r--r--r--). Rescue this error and use cp_r's\n            # :remove_destination option.\n            begin\n              FileUtils.cp(source_file, \"#{destination}/#{relative_path}\")\n            rescue Errno::EACCES\n              FileUtils.cp_r(source_file, \"#{destination}/#{relative_path}\", remove_destination: true)\n            end\n          end\n        else\n          raise \"Unknown file type: `File.ftype(source_file)' at `#{source_file}'!\"\n        end\n      end\n\n      # Remove any files in the destination that are not in the source files\n      destination_files = glob(\"#{destination}/**/*\")\n\n      # Calculate the relative paths of files so we can compare to the\n      # source.\n      relative_source_files = source_files.map do |file|\n        relative_path_for(file, source)\n      end\n      relative_destination_files = destination_files.map do |file|\n        relative_path_for(file, destination)\n      end\n\n      # Remove any extra files that are present in the destination, but are\n      # not in the source list\n      extra_files = relative_destination_files - relative_source_files\n      extra_files.each do |file|\n        FileUtils.rm_rf(File.join(destination, file))\n      end\n\n      true\n    end", "language": "ruby", "code": "def sync(source, destination, options = {})\n      unless File.directory?(source)\n        raise ArgumentError, \"`source' must be a directory, but was a \" \\\n          \"`#{File.ftype(source)}'! If you just want to sync a file, use \" \\\n          \"the `copy' method instead.\"\n      end\n\n      source_files = all_files_under(source, options)\n\n      # Ensure the destination directory exists\n      FileUtils.mkdir_p(destination) unless File.directory?(destination)\n\n      # Copy over the filtered source files\n      source_files.each do |source_file|\n        relative_path = relative_path_for(source_file, source)\n\n        # Create the parent directory\n        parent = File.join(destination, File.dirname(relative_path))\n        FileUtils.mkdir_p(parent) unless File.directory?(parent)\n\n        case File.ftype(source_file).to_sym\n        when :directory\n          FileUtils.mkdir_p(\"#{destination}/#{relative_path}\")\n        when :link\n          target = File.readlink(source_file)\n\n          Dir.chdir(destination) do\n            FileUtils.ln_sf(target, \"#{destination}/#{relative_path}\")\n          end\n        when :file\n          source_stat = File.stat(source_file)\n          # Detect 'files' which are hard links and use ln instead of cp to\n          # duplicate them, provided their source is in place already\n          if hardlink? source_stat\n            if existing = hardlink_sources[[source_stat.dev, source_stat.ino]]\n              FileUtils.ln(existing, \"#{destination}/#{relative_path}\", force: true)\n            else\n              begin\n                FileUtils.cp(source_file, \"#{destination}/#{relative_path}\")\n              rescue Errno::EACCES\n                FileUtils.cp_r(source_file, \"#{destination}/#{relative_path}\", remove_destination: true)\n              end\n              hardlink_sources.store([source_stat.dev, source_stat.ino], \"#{destination}/#{relative_path}\")\n            end\n          else\n            # First attempt a regular copy. If we don't have write\n            # permission on the File, open will probably fail with\n            # EACCES (making it hard to sync files with permission\n            # r--r--r--). Rescue this error and use cp_r's\n            # :remove_destination option.\n            begin\n              FileUtils.cp(source_file, \"#{destination}/#{relative_path}\")\n            rescue Errno::EACCES\n              FileUtils.cp_r(source_file, \"#{destination}/#{relative_path}\", remove_destination: true)\n            end\n          end\n        else\n          raise \"Unknown file type: `File.ftype(source_file)' at `#{source_file}'!\"\n        end\n      end\n\n      # Remove any files in the destination that are not in the source files\n      destination_files = glob(\"#{destination}/**/*\")\n\n      # Calculate the relative paths of files so we can compare to the\n      # source.\n      relative_source_files = source_files.map do |file|\n        relative_path_for(file, source)\n      end\n      relative_destination_files = destination_files.map do |file|\n        relative_path_for(file, destination)\n      end\n\n      # Remove any extra files that are present in the destination, but are\n      # not in the source list\n      extra_files = relative_destination_files - relative_source_files\n      extra_files.each do |file|\n        FileUtils.rm_rf(File.join(destination, file))\n      end\n\n      true\n    end", "code_tokens": ["def", "sync", "(", "source", ",", "destination", ",", "options", "=", "{", "}", ")", "unless", "File", ".", "directory?", "(", "source", ")", "raise", "ArgumentError", ",", "\"`source' must be a directory, but was a \"", "\"`#{File.ftype(source)}'! If you just want to sync a file, use \"", "\"the `copy' method instead.\"", "end", "source_files", "=", "all_files_under", "(", "source", ",", "options", ")", "# Ensure the destination directory exists", "FileUtils", ".", "mkdir_p", "(", "destination", ")", "unless", "File", ".", "directory?", "(", "destination", ")", "# Copy over the filtered source files", "source_files", ".", "each", "do", "|", "source_file", "|", "relative_path", "=", "relative_path_for", "(", "source_file", ",", "source", ")", "# Create the parent directory", "parent", "=", "File", ".", "join", "(", "destination", ",", "File", ".", "dirname", "(", "relative_path", ")", ")", "FileUtils", ".", "mkdir_p", "(", "parent", ")", "unless", "File", ".", "directory?", "(", "parent", ")", "case", "File", ".", "ftype", "(", "source_file", ")", ".", "to_sym", "when", ":directory", "FileUtils", ".", "mkdir_p", "(", "\"#{destination}/#{relative_path}\"", ")", "when", ":link", "target", "=", "File", ".", "readlink", "(", "source_file", ")", "Dir", ".", "chdir", "(", "destination", ")", "do", "FileUtils", ".", "ln_sf", "(", "target", ",", "\"#{destination}/#{relative_path}\"", ")", "end", "when", ":file", "source_stat", "=", "File", ".", "stat", "(", "source_file", ")", "# Detect 'files' which are hard links and use ln instead of cp to", "# duplicate them, provided their source is in place already", "if", "hardlink?", "source_stat", "if", "existing", "=", "hardlink_sources", "[", "[", "source_stat", ".", "dev", ",", "source_stat", ".", "ino", "]", "]", "FileUtils", ".", "ln", "(", "existing", ",", "\"#{destination}/#{relative_path}\"", ",", "force", ":", "true", ")", "else", "begin", "FileUtils", ".", "cp", "(", "source_file", ",", "\"#{destination}/#{relative_path}\"", ")", "rescue", "Errno", "::", "EACCES", "FileUtils", ".", "cp_r", "(", "source_file", ",", "\"#{destination}/#{relative_path}\"", ",", "remove_destination", ":", "true", ")", "end", "hardlink_sources", ".", "store", "(", "[", "source_stat", ".", "dev", ",", "source_stat", ".", "ino", "]", ",", "\"#{destination}/#{relative_path}\"", ")", "end", "else", "# First attempt a regular copy. If we don't have write", "# permission on the File, open will probably fail with", "# EACCES (making it hard to sync files with permission", "# r--r--r--). Rescue this error and use cp_r's", "# :remove_destination option.", "begin", "FileUtils", ".", "cp", "(", "source_file", ",", "\"#{destination}/#{relative_path}\"", ")", "rescue", "Errno", "::", "EACCES", "FileUtils", ".", "cp_r", "(", "source_file", ",", "\"#{destination}/#{relative_path}\"", ",", "remove_destination", ":", "true", ")", "end", "end", "else", "raise", "\"Unknown file type: `File.ftype(source_file)' at `#{source_file}'!\"", "end", "end", "# Remove any files in the destination that are not in the source files", "destination_files", "=", "glob", "(", "\"#{destination}/**/*\"", ")", "# Calculate the relative paths of files so we can compare to the", "# source.", "relative_source_files", "=", "source_files", ".", "map", "do", "|", "file", "|", "relative_path_for", "(", "file", ",", "source", ")", "end", "relative_destination_files", "=", "destination_files", ".", "map", "do", "|", "file", "|", "relative_path_for", "(", "file", ",", "destination", ")", "end", "# Remove any extra files that are present in the destination, but are", "# not in the source list", "extra_files", "=", "relative_destination_files", "-", "relative_source_files", "extra_files", ".", "each", "do", "|", "file", "|", "FileUtils", ".", "rm_rf", "(", "File", ".", "join", "(", "destination", ",", "file", ")", ")", "end", "true", "end"], "docstring": "Copy the files from +source+ to +destination+, while removing any files\n in +destination+ that are not present in +source+.\n\n The method accepts an optional +:exclude+ parameter to ignore files and\n folders that match the given pattern(s). Note the exclude pattern behaves\n on paths relative to the given source. If you want to exclude a nested\n directory, you will need to use something like +**/directory+.\n\n @raise ArgumentError\n   if the +source+ parameter is not a directory\n\n @param [String] source\n   the path on disk to sync from\n @param [String] destination\n   the path on disk to sync to\n\n @option options [String, Array<String>] :exclude\n   a file, folder, or globbing pattern of files to ignore when syncing\n\n @return [true]", "docstring_tokens": ["Copy", "the", "files", "from", "+", "source", "+", "to", "+", "destination", "+", "while", "removing", "any", "files", "in", "+", "destination", "+", "that", "are", "not", "present", "in", "+", "source", "+", "."], "sha": "968307c129ee54416f5a4d07ca8f8ca2d2b12825", "url": "https://github.com/chef/omnibus/blob/968307c129ee54416f5a4d07ca8f8ca2d2b12825/lib/omnibus/file_syncer.rb#L91-L172"}, "target": "Copy the files from + source + to + destination + while removing any files in + destination + that are not present in + source + .", "arguments": {"gen_args_0": {"arg_0": "def sync ( source , destination , options = { } ) unless File . directory? ( source ) raise ArgumentError , \"`source' must be a directory, but was a \" \"`#{File.ftype(source)}'! If you just want to sync a file, use \" \"the `copy' method instead.\" end source_files = all_files_under ( source , options ) # Ensure the destination directory exists FileUtils . mkdir_p ( destination ) unless File . directory? ( destination ) # Copy over the filtered source files source_files . each do | source_file | relative_path = relative_path_for ( source_file , source ) # Create the parent directory parent = File . join ( destination , File . dirname ( relative_path ) ) FileUtils . mkdir_p ( parent ) unless File . directory? ( parent ) case File . ftype ( source_file ) . to_sym when :directory FileUtils . mkdir_p ( \"#{destination}/#{relative_path}\" ) when :link target = File . readlink ( source_file ) Dir . chdir ( destination ) do FileUtils . ln_sf ( target , \"#{destination}/#{relative_path}\" ) end when :file source_stat = File . stat ( source_file ) # Detect 'files' which are hard links and use ln instead of cp to # duplicate them, provided their source is in place already if hardlink? source_stat if existing = hardlink_sources [ [ source_stat . dev , source_stat . ino ] ] FileUtils . ln ( existing , \"#{destination}/#{relative_path}\" , force : true ) else begin FileUtils . cp ( source_file , \"#{destination}/#{relative_path}\" ) rescue Errno :: EACCES FileUtils . cp_r ( source_file , \"#{destination}/#{relative_path}\" , remove_destination : true ) end hardlink_sources . store ( [ source_stat . dev , source_stat . ino ] , \"#{destination}/#{relative_path}\" ) end else # First attempt a regular copy. If we don't have write # permission on the File, open will probably fail with # EACCES (making it hard to sync files with permission # r--r--r--). Rescue this error and use cp_r's # :remove_destination option. begin FileUtils . cp ( source_file , \"#{destination}/#{relative_path}\" ) rescue Errno :: EACCES FileUtils . cp_r ( source_file , \"#{destination}/#{relative_path}\" , remove_destination : true ) end end else raise \"Unknown file type: `File.ftype(source_file)' at `#{source_file}'!\" end end # Remove any files in the destination that are not in the source files destination_files = glob ( \"#{destination}/**/*\" ) # Calculate the relative paths of files so we can compare to the # source. relative_source_files = source_files . map do | file | relative_path_for ( file , source ) end relative_destination_files = destination_files . map do | file | relative_path_for ( file , destination ) end # Remove any extra files that are present in the destination, but are # not in the source list extra_files = relative_destination_files - relative_source_files extra_files . each do | file | FileUtils . rm_rf ( File . join ( destination , file ) ) end true end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def all_files_under ( path , options = { } ) files ="]], "filtered_resps": ["def all_files_under ( path , options = { } ) files ="], "doc_hash": "37f3f7b6999e532a9665a9f9960b0622a31f91a78c3afaf898b0f052aeb33126", "prompt_hash": "c4fc309395741675bff591d69db0b4342f66e8956efd1a51e9b086cc1496b223", "target_hash": "990113c5b2c85fc68b072283afe77f615601a33e44b8a3f65cccc1252474e97f", "smoothed_bleu_4": 3.8555103485489663}
{"doc_id": 7, "doc": {"id": 7, "repo": "chef/omnibus", "path": "lib/omnibus/file_syncer.rb", "func_name": "Omnibus.FileSyncer.relative_path_for", "original_string": "def relative_path_for(path, parent)\n      Pathname.new(path).relative_path_from(Pathname.new(parent)).to_s\n    end", "language": "ruby", "code": "def relative_path_for(path, parent)\n      Pathname.new(path).relative_path_from(Pathname.new(parent)).to_s\n    end", "code_tokens": ["def", "relative_path_for", "(", "path", ",", "parent", ")", "Pathname", ".", "new", "(", "path", ")", ".", "relative_path_from", "(", "Pathname", ".", "new", "(", "parent", ")", ")", ".", "to_s", "end"], "docstring": "The relative path of the given +path+ to the +parent+.\n\n @param [String] path\n   the path to get relative with\n @param [String] parent\n   the parent where the path is contained (hopefully)\n\n @return [String]", "docstring_tokens": ["The", "relative", "path", "of", "the", "given", "+", "path", "+", "to", "the", "+", "parent", "+", "."], "sha": "968307c129ee54416f5a4d07ca8f8ca2d2b12825", "url": "https://github.com/chef/omnibus/blob/968307c129ee54416f5a4d07ca8f8ca2d2b12825/lib/omnibus/file_syncer.rb#L186-L188"}, "target": "The relative path of the given + path + to the + parent + .", "arguments": {"gen_args_0": {"arg_0": "def relative_path_for ( path , parent ) Pathname . new ( path ) . relative_path_from ( Pathname . new ( parent ) ) . to_s end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def absolute_path_for ( path , parent ) Pathname ."]], "filtered_resps": ["def absolute_path_for ( path , parent ) Pathname ."], "doc_hash": "38ce73d394536e3438a972e63ac4c2f6a2b578302452e6357b8875525515fc41", "prompt_hash": "8dfc8565daee0d43aeec5deb85b62439e5403d5558c999c912dfe1025638b64a", "target_hash": "e9cefff79e6946fba88a5f524c2c55abaf13341060f50e2778096724fa555090", "smoothed_bleu_4": 10.03131273265709}
{"doc_id": 8, "doc": {"id": 8, "repo": "chef/omnibus", "path": "lib/omnibus/compressors/dmg.rb", "func_name": "Omnibus.Compressor::DMG.clean_disks", "original_string": "def clean_disks\n      log.info(log_key) { \"Cleaning previously mounted disks\" }\n\n      existing_disks = shellout!(\"mount | grep \\\"/Volumes/#{volume_name}\\\" | awk '{print $1}'\")\n      existing_disks.stdout.lines.each do |existing_disk|\n        existing_disk.chomp!\n\n        Omnibus.logger.debug(log_key) do\n          \"Detaching disk `#{existing_disk}' before starting dmg packaging.\"\n        end\n\n        shellout!(\"hdiutil detach '#{existing_disk}'\")\n      end\n    end", "language": "ruby", "code": "def clean_disks\n      log.info(log_key) { \"Cleaning previously mounted disks\" }\n\n      existing_disks = shellout!(\"mount | grep \\\"/Volumes/#{volume_name}\\\" | awk '{print $1}'\")\n      existing_disks.stdout.lines.each do |existing_disk|\n        existing_disk.chomp!\n\n        Omnibus.logger.debug(log_key) do\n          \"Detaching disk `#{existing_disk}' before starting dmg packaging.\"\n        end\n\n        shellout!(\"hdiutil detach '#{existing_disk}'\")\n      end\n    end", "code_tokens": ["def", "clean_disks", "log", ".", "info", "(", "log_key", ")", "{", "\"Cleaning previously mounted disks\"", "}", "existing_disks", "=", "shellout!", "(", "\"mount | grep \\\"/Volumes/#{volume_name}\\\" | awk '{print $1}'\"", ")", "existing_disks", ".", "stdout", ".", "lines", ".", "each", "do", "|", "existing_disk", "|", "existing_disk", ".", "chomp!", "Omnibus", ".", "logger", ".", "debug", "(", "log_key", ")", "do", "\"Detaching disk `#{existing_disk}' before starting dmg packaging.\"", "end", "shellout!", "(", "\"hdiutil detach '#{existing_disk}'\"", ")", "end", "end"], "docstring": "Cleans any previously left over mounted disks.\n\n We are trying to detach disks that look like:\n\n   /dev/disk1s1 on /Volumes/chef (hfs, local, nodev, nosuid, read-only, noowners, quarantine, mounted by serdar)\n   /dev/disk2s1 on /Volumes/chef 1 (hfs, local, nodev, nosuid, read-only, noowners, quarantine, mounted by serdar)\n\n @return [void]", "docstring_tokens": ["Cleans", "any", "previously", "left", "over", "mounted", "disks", "."], "sha": "968307c129ee54416f5a4d07ca8f8ca2d2b12825", "url": "https://github.com/chef/omnibus/blob/968307c129ee54416f5a4d07ca8f8ca2d2b12825/lib/omnibus/compressors/dmg.rb#L119-L132"}, "target": "Cleans any previously left over mounted disks .", "arguments": {"gen_args_0": {"arg_0": "def clean_disks log . info ( log_key ) { \"Cleaning previously mounted disks\" } existing_disks = shellout! ( \"mount | grep \\\"/Volumes/#{volume_name}\\\" | awk '{print $1}'\" ) existing_disks . stdout . lines . each do | existing_disk | existing_disk . chomp! Omnibus . logger . debug ( log_key ) do \"Detaching disk `#{existing_disk}' before starting dmg packaging.\" end shellout! ( \"hdiutil detach '#{existing_disk}'\" ) end end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [[""]], "filtered_resps": [""], "doc_hash": "ec8c65f0a219a6c9429df1d65fd3f0e137d4178b57b61eff93e19e3b3433e453", "prompt_hash": "e5d0d1af495edf2d8290b5b77dc6de4d215da5ef5d49f6bee9fdaf7340d6b23e", "target_hash": "4ab6f54b640f11e7976d671153bc2da59c1cac83734b32f3fd23204a417eae3c", "smoothed_bleu_4": 0.033546262790251184}
{"doc_id": 9, "doc": {"id": 9, "repo": "chef/omnibus", "path": "lib/omnibus/compressors/dmg.rb", "func_name": "Omnibus.Compressor::DMG.copy_assets_to_dmg", "original_string": "def copy_assets_to_dmg\n      log.info(log_key) { \"Copying assets into dmg\" }\n\n      FileSyncer.glob(\"#{resources_dir}/*\").each do |file|\n        FileUtils.cp_r(file, \"/Volumes/#{volume_name}\")\n      end\n    end", "language": "ruby", "code": "def copy_assets_to_dmg\n      log.info(log_key) { \"Copying assets into dmg\" }\n\n      FileSyncer.glob(\"#{resources_dir}/*\").each do |file|\n        FileUtils.cp_r(file, \"/Volumes/#{volume_name}\")\n      end\n    end", "code_tokens": ["def", "copy_assets_to_dmg", "log", ".", "info", "(", "log_key", ")", "{", "\"Copying assets into dmg\"", "}", "FileSyncer", ".", "glob", "(", "\"#{resources_dir}/*\"", ")", ".", "each", "do", "|", "file", "|", "FileUtils", ".", "cp_r", "(", "file", ",", "\"/Volumes/#{volume_name}\"", ")", "end", "end"], "docstring": "Copy assets to dmg", "docstring_tokens": ["Copy", "assets", "to", "dmg"], "sha": "968307c129ee54416f5a4d07ca8f8ca2d2b12825", "url": "https://github.com/chef/omnibus/blob/968307c129ee54416f5a4d07ca8f8ca2d2b12825/lib/omnibus/compressors/dmg.rb#L175-L181"}, "target": "Copy assets to dmg", "arguments": {"gen_args_0": {"arg_0": "def copy_assets_to_dmg log . info ( log_key ) { \"Copying assets into dmg\" } FileSyncer . glob ( \"#{resources_dir}/*\" ) . each do | file | FileUtils . cp_r ( file , \"/Volumes/#{volume_name}\" ) end end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def create_dmg log . info ( log_key ) {"]], "filtered_resps": ["def create_dmg log . info ( log_key ) {"], "doc_hash": "7fb1012ec916d028d61cdc816d1efa6062788cb1ee712170563372bf155737c7", "prompt_hash": "776af8897aaa184e05320c160f922af866a994fc3b67266837c237dd9e9aff3a", "target_hash": "6e4545c0e008c00a8754e2b1ae0faafc0f36ceec2a24f62c1861397952802fff", "smoothed_bleu_4": 8.182477900877295}
{"doc_id": 10, "doc": {"id": 10, "repo": "chef/omnibus", "path": "lib/omnibus/packagers/ips.rb", "func_name": "Omnibus.Packager::IPS.write_transform_file", "original_string": "def write_transform_file\n      render_template(resource_path(\"doc-transform.erb\"),\n        destination: transform_file,\n        variables: {\n          pathdir: project.install_dir.split(\"/\")[1],\n        }\n      )\n    end", "language": "ruby", "code": "def write_transform_file\n      render_template(resource_path(\"doc-transform.erb\"),\n        destination: transform_file,\n        variables: {\n          pathdir: project.install_dir.split(\"/\")[1],\n        }\n      )\n    end", "code_tokens": ["def", "write_transform_file", "render_template", "(", "resource_path", "(", "\"doc-transform.erb\"", ")", ",", "destination", ":", "transform_file", ",", "variables", ":", "{", "pathdir", ":", "project", ".", "install_dir", ".", "split", "(", "\"/\"", ")", "[", "1", "]", ",", "}", ")", "end"], "docstring": "A set of transform rules that `pkgmogrify' will apply to the package\n manifest.\n\n @return [void]", "docstring_tokens": ["A", "set", "of", "transform", "rules", "that", "pkgmogrify", "will", "apply", "to", "the", "package", "manifest", "."], "sha": "968307c129ee54416f5a4d07ca8f8ca2d2b12825", "url": "https://github.com/chef/omnibus/blob/968307c129ee54416f5a4d07ca8f8ca2d2b12825/lib/omnibus/packagers/ips.rb#L207-L214"}, "target": "A set of transform rules that pkgmogrify will apply to the package manifest .", "arguments": {"gen_args_0": {"arg_0": "def write_transform_file render_template ( resource_path ( \"doc-transform.erb\" ) , destination : transform_file , variables : { pathdir : project . install_dir . split ( \"/\" ) [ 1 ] , } ) end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def write_config_file render_template ( resource_path ("]], "filtered_resps": ["def write_config_file render_template ( resource_path ("], "doc_hash": "08abf1f5d66a19af40d0be94ef7c346581622d204d403a108d29777d3476b1ec", "prompt_hash": "b94931cf314efc600c338da2aaa9cd7c20a677b8d0f3ece638d6b4958ca0c80b", "target_hash": "9391e19007df99dcd5b144b58fc040579e185781c81c29edd5bba746ec3f6740", "smoothed_bleu_4": 9.236149212030169e-77}
{"doc_id": 11, "doc": {"id": 11, "repo": "chef/omnibus", "path": "lib/omnibus/packagers/ips.rb", "func_name": "Omnibus.Packager::IPS.write_pkg_metadata", "original_string": "def write_pkg_metadata\n      render_template(resource_path(\"gen.manifestfile.erb\"),\n        destination: pkg_metadata_file,\n        variables: {\n          name: safe_base_package_name,\n          fmri_package_name: fmri_package_name,\n          description: project.description,\n          summary: project.friendly_name,\n          arch: safe_architecture,\n        }\n      )\n\n      # Append the contents of symlinks_file if it exists\n      if symlinks_file\n        File.open(pkg_metadata_file, \"a\") do |symlink|\n          symlink.write(render_symlinks)\n        end\n      end\n\n      # Print the full contents of the rendered template file to generate package contents\n      log.debug(log_key) { \"Rendered Template:\\n\" + File.read(pkg_metadata_file) }\n    end", "language": "ruby", "code": "def write_pkg_metadata\n      render_template(resource_path(\"gen.manifestfile.erb\"),\n        destination: pkg_metadata_file,\n        variables: {\n          name: safe_base_package_name,\n          fmri_package_name: fmri_package_name,\n          description: project.description,\n          summary: project.friendly_name,\n          arch: safe_architecture,\n        }\n      )\n\n      # Append the contents of symlinks_file if it exists\n      if symlinks_file\n        File.open(pkg_metadata_file, \"a\") do |symlink|\n          symlink.write(render_symlinks)\n        end\n      end\n\n      # Print the full contents of the rendered template file to generate package contents\n      log.debug(log_key) { \"Rendered Template:\\n\" + File.read(pkg_metadata_file) }\n    end", "code_tokens": ["def", "write_pkg_metadata", "render_template", "(", "resource_path", "(", "\"gen.manifestfile.erb\"", ")", ",", "destination", ":", "pkg_metadata_file", ",", "variables", ":", "{", "name", ":", "safe_base_package_name", ",", "fmri_package_name", ":", "fmri_package_name", ",", "description", ":", "project", ".", "description", ",", "summary", ":", "project", ".", "friendly_name", ",", "arch", ":", "safe_architecture", ",", "}", ")", "# Append the contents of symlinks_file if it exists", "if", "symlinks_file", "File", ".", "open", "(", "pkg_metadata_file", ",", "\"a\"", ")", "do", "|", "symlink", "|", "symlink", ".", "write", "(", "render_symlinks", ")", "end", "end", "# Print the full contents of the rendered template file to generate package contents", "log", ".", "debug", "(", "log_key", ")", "{", "\"Rendered Template:\\n\"", "+", "File", ".", "read", "(", "pkg_metadata_file", ")", "}", "end"], "docstring": "Generate package metadata\n\n Create the gen template for `pkgmogrify`\n\n @return [void]", "docstring_tokens": ["Generate", "package", "metadata"], "sha": "968307c129ee54416f5a4d07ca8f8ca2d2b12825", "url": "https://github.com/chef/omnibus/blob/968307c129ee54416f5a4d07ca8f8ca2d2b12825/lib/omnibus/packagers/ips.rb#L256-L277"}, "target": "Generate package metadata", "arguments": {"gen_args_0": {"arg_0": "def write_pkg_metadata render_template ( resource_path ( \"gen.manifestfile.erb\" ) , destination : pkg_metadata_file , variables : { name : safe_base_package_name , fmri_package_name : fmri_package_name , description : project . description , summary : project . friendly_name , arch : safe_architecture , } ) # Append the contents of symlinks_file if it exists if symlinks_file File . open ( pkg_metadata_file , \"a\" ) do | symlink | symlink . write ( render_symlinks ) end end # Print the full contents of the rendered template file to generate package contents log . debug ( log_key ) { \"Rendered Template:\\n\" + File . read ( pkg_metadata_file ) } end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [[""]], "filtered_resps": [""], "doc_hash": "c43f7861830393b1c74eeaac4660f123a4c234af5b785f0d76b26bedd7bb8fb6", "prompt_hash": "4f5008d3d79d96ae0c02e09b9ea34b8cdda661b19cdb7b9bbc8ec8e15e63b813", "target_hash": "9a3c1a220bd95ebfb1bb10626d2d3af78b55613a1b2dcb72ec01b9923bccaf2a", "smoothed_bleu_4": 4.978706836786395}
{"doc_id": 12, "doc": {"id": 12, "repo": "chef/omnibus", "path": "lib/omnibus/health_check.rb", "func_name": "Omnibus.HealthCheck.health_check_otool", "original_string": "def health_check_otool\n      current_library = nil\n      bad_libs = {}\n\n      read_shared_libs(\"find #{project.install_dir}/ -type f | egrep '\\.(dylib|bundle)$' | xargs otool -L\") do |line|\n        case line\n        when /^(.+):$/\n          current_library = Regexp.last_match[1]\n        when /^\\s+(.+) \\(.+\\)$/\n          linked = Regexp.last_match[1]\n          name = File.basename(linked)\n          bad_libs = check_for_bad_library(bad_libs, current_library, name, linked)\n        end\n      end\n\n      bad_libs\n    end", "language": "ruby", "code": "def health_check_otool\n      current_library = nil\n      bad_libs = {}\n\n      read_shared_libs(\"find #{project.install_dir}/ -type f | egrep '\\.(dylib|bundle)$' | xargs otool -L\") do |line|\n        case line\n        when /^(.+):$/\n          current_library = Regexp.last_match[1]\n        when /^\\s+(.+) \\(.+\\)$/\n          linked = Regexp.last_match[1]\n          name = File.basename(linked)\n          bad_libs = check_for_bad_library(bad_libs, current_library, name, linked)\n        end\n      end\n\n      bad_libs\n    end", "code_tokens": ["def", "health_check_otool", "current_library", "=", "nil", "bad_libs", "=", "{", "}", "read_shared_libs", "(", "\"find #{project.install_dir}/ -type f | egrep '\\.(dylib|bundle)$' | xargs otool -L\"", ")", "do", "|", "line", "|", "case", "line", "when", "/", "/", "current_library", "=", "Regexp", ".", "last_match", "[", "1", "]", "when", "/", "\\s", "\\(", "\\)", "/", "linked", "=", "Regexp", ".", "last_match", "[", "1", "]", "name", "=", "File", ".", "basename", "(", "linked", ")", "bad_libs", "=", "check_for_bad_library", "(", "bad_libs", ",", "current_library", ",", "name", ",", "linked", ")", "end", "end", "bad_libs", "end"], "docstring": "Run healthchecks against otool.\n\n @return [Hash<String, Hash<String, Hash<String, Int>>>]\n   the bad libraries (library_name -> dependency_name -> satisfied_lib_path -> count)", "docstring_tokens": ["Run", "healthchecks", "against", "otool", "."], "sha": "968307c129ee54416f5a4d07ca8f8ca2d2b12825", "url": "https://github.com/chef/omnibus/blob/968307c129ee54416f5a4d07ca8f8ca2d2b12825/lib/omnibus/health_check.rb#L280-L296"}, "target": "Run healthchecks against otool .", "arguments": {"gen_args_0": {"arg_0": "def health_check_otool current_library = nil bad_libs = { } read_shared_libs ( \"find #{project.install_dir}/ -type f | egrep '\\.(dylib|bundle)$' | xargs otool -L\" ) do | line | case line when / / current_library = Regexp . last_match [ 1 ] when / \\s \\( \\) / linked = Regexp . last_match [ 1 ] name = File . basename ( linked ) bad_libs = check_for_bad_library ( bad_libs , current_library , name , linked ) end end bad_libs end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def check_for_bad_library ( bad_libs , current_"]], "filtered_resps": ["def check_for_bad_library ( bad_libs , current_"], "doc_hash": "ae58e1b1c7b682943da8690f4cd67578bc3e47716666ad7b2a6aaa7c5a77de22", "prompt_hash": "e230f6988cda33c830bc2e99a5b5f4ceb9386f60a1ec62ad83b48a6d9ab9824a", "target_hash": "ac83044d1f34a1f4dcd9ed0e3530b70bbde5a00fe7404325e27223b9274a0d4f", "smoothed_bleu_4": 8.585639149516246e-77}
{"doc_id": 13, "doc": {"id": 13, "repo": "chef/omnibus", "path": "lib/omnibus/health_check.rb", "func_name": "Omnibus.HealthCheck.health_check_aix", "original_string": "def health_check_aix\n      current_library = nil\n      bad_libs = {}\n\n      read_shared_libs(\"find #{project.install_dir}/ -type f | xargs file | grep \\\"RISC System\\\" | awk -F: '{print $1}' | xargs -n 1 ldd\") do |line|\n        case line\n        when /^(.+) needs:$/\n          current_library = Regexp.last_match[1]\n          log.debug(log_key) { \"Analyzing dependencies for #{current_library}\" }\n        when /^\\s+(.+)$/\n          name = Regexp.last_match[1]\n          linked = Regexp.last_match[1]\n          bad_libs = check_for_bad_library(bad_libs, current_library, name, linked)\n        when /File is not an executable XCOFF file/ # ignore non-executable files\n        else\n          log.warn(log_key) { \"Line did not match for #{current_library}\\n#{line}\" }\n        end\n      end\n\n      bad_libs\n    end", "language": "ruby", "code": "def health_check_aix\n      current_library = nil\n      bad_libs = {}\n\n      read_shared_libs(\"find #{project.install_dir}/ -type f | xargs file | grep \\\"RISC System\\\" | awk -F: '{print $1}' | xargs -n 1 ldd\") do |line|\n        case line\n        when /^(.+) needs:$/\n          current_library = Regexp.last_match[1]\n          log.debug(log_key) { \"Analyzing dependencies for #{current_library}\" }\n        when /^\\s+(.+)$/\n          name = Regexp.last_match[1]\n          linked = Regexp.last_match[1]\n          bad_libs = check_for_bad_library(bad_libs, current_library, name, linked)\n        when /File is not an executable XCOFF file/ # ignore non-executable files\n        else\n          log.warn(log_key) { \"Line did not match for #{current_library}\\n#{line}\" }\n        end\n      end\n\n      bad_libs\n    end", "code_tokens": ["def", "health_check_aix", "current_library", "=", "nil", "bad_libs", "=", "{", "}", "read_shared_libs", "(", "\"find #{project.install_dir}/ -type f | xargs file | grep \\\"RISC System\\\" | awk -F: '{print $1}' | xargs -n 1 ldd\"", ")", "do", "|", "line", "|", "case", "line", "when", "/", "/", "current_library", "=", "Regexp", ".", "last_match", "[", "1", "]", "log", ".", "debug", "(", "log_key", ")", "{", "\"Analyzing dependencies for #{current_library}\"", "}", "when", "/", "\\s", "/", "name", "=", "Regexp", ".", "last_match", "[", "1", "]", "linked", "=", "Regexp", ".", "last_match", "[", "1", "]", "bad_libs", "=", "check_for_bad_library", "(", "bad_libs", ",", "current_library", ",", "name", ",", "linked", ")", "when", "/", "/", "# ignore non-executable files", "else", "log", ".", "warn", "(", "log_key", ")", "{", "\"Line did not match for #{current_library}\\n#{line}\"", "}", "end", "end", "bad_libs", "end"], "docstring": "Run healthchecks against aix.\n\n @return [Hash<String, Hash<String, Hash<String, Int>>>]\n   the bad libraries (library_name -> dependency_name -> satisfied_lib_path -> count)", "docstring_tokens": ["Run", "healthchecks", "against", "aix", "."], "sha": "968307c129ee54416f5a4d07ca8f8ca2d2b12825", "url": "https://github.com/chef/omnibus/blob/968307c129ee54416f5a4d07ca8f8ca2d2b12825/lib/omnibus/health_check.rb#L304-L324"}, "target": "Run healthchecks against aix .", "arguments": {"gen_args_0": {"arg_0": "def health_check_aix current_library = nil bad_libs = { } read_shared_libs ( \"find #{project.install_dir}/ -type f | xargs file | grep \\\"RISC System\\\" | awk -F: '{print $1}' | xargs -n 1 ldd\" ) do | line | case line when / / current_library = Regexp . last_match [ 1 ] log . debug ( log_key ) { \"Analyzing dependencies for #{current_library}\" } when / \\s / name = Regexp . last_match [ 1 ] linked = Regexp . last_match [ 1 ] bad_libs = check_for_bad_library ( bad_libs , current_library , name , linked ) when / / # ignore non-executable files else log . warn ( log_key ) { \"Line did not match for #{current_library}\\n#{line}\" } end end bad_libs end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def check_for_bad_library ( bad_libs , current_"]], "filtered_resps": ["def check_for_bad_library ( bad_libs , current_"], "doc_hash": "898e0cda6fb3073aba9356069b53e72d8dd0f076b1c7936701b6d517761c37c0", "prompt_hash": "be3c9b3eb930f3c44bf2c359c9426f944cad5db877459dc76976bff07066ec5a", "target_hash": "b0ec8334ec6e5f6a3e0bd9d0b3042ef1ac32a5f7281b847e419ff0e02fed4898", "smoothed_bleu_4": 8.585639149516246e-77}
{"doc_id": 14, "doc": {"id": 14, "repo": "chef/omnibus", "path": "lib/omnibus/health_check.rb", "func_name": "Omnibus.HealthCheck.health_check_ldd", "original_string": "def health_check_ldd\n      regexp_ends = \".*(\" + IGNORED_ENDINGS.map { |e| e.gsub(/\\./, '\\.') }.join(\"|\") + \")$\"\n      regexp_patterns = IGNORED_PATTERNS.map { |e| \".*\" + e.gsub(/\\//, '\\/') + \".*\" }.join(\"|\")\n      regexp = regexp_ends + \"|\" + regexp_patterns\n\n      current_library = nil\n      bad_libs = {}\n\n      read_shared_libs(\"find #{project.install_dir}/ -type f -regextype posix-extended ! -regex '#{regexp}' | xargs ldd\") do |line|\n        case line\n        when /^(.+):$/\n          current_library = Regexp.last_match[1]\n          log.debug(log_key) { \"Analyzing dependencies for #{current_library}\" }\n        when /^\\s+(.+) \\=\\>\\s+(.+)( \\(.+\\))?$/\n          name = Regexp.last_match[1]\n          linked = Regexp.last_match[2]\n          bad_libs = check_for_bad_library(bad_libs, current_library, name, linked)\n        when /^\\s+(.+) \\(.+\\)$/\n          next\n        when /^\\s+statically linked$/\n          next\n        when /^\\s+libjvm.so/\n          next\n        when /^\\s+libjava.so/\n          next\n        when /^\\s+libmawt.so/\n          next\n        when /^\\s+not a dynamic executable$/ # ignore non-executable files\n        else\n          log.warn(log_key) do\n            \"Line did not match for #{current_library}\\n#{line}\"\n          end\n        end\n      end\n\n      bad_libs\n    end", "language": "ruby", "code": "def health_check_ldd\n      regexp_ends = \".*(\" + IGNORED_ENDINGS.map { |e| e.gsub(/\\./, '\\.') }.join(\"|\") + \")$\"\n      regexp_patterns = IGNORED_PATTERNS.map { |e| \".*\" + e.gsub(/\\//, '\\/') + \".*\" }.join(\"|\")\n      regexp = regexp_ends + \"|\" + regexp_patterns\n\n      current_library = nil\n      bad_libs = {}\n\n      read_shared_libs(\"find #{project.install_dir}/ -type f -regextype posix-extended ! -regex '#{regexp}' | xargs ldd\") do |line|\n        case line\n        when /^(.+):$/\n          current_library = Regexp.last_match[1]\n          log.debug(log_key) { \"Analyzing dependencies for #{current_library}\" }\n        when /^\\s+(.+) \\=\\>\\s+(.+)( \\(.+\\))?$/\n          name = Regexp.last_match[1]\n          linked = Regexp.last_match[2]\n          bad_libs = check_for_bad_library(bad_libs, current_library, name, linked)\n        when /^\\s+(.+) \\(.+\\)$/\n          next\n        when /^\\s+statically linked$/\n          next\n        when /^\\s+libjvm.so/\n          next\n        when /^\\s+libjava.so/\n          next\n        when /^\\s+libmawt.so/\n          next\n        when /^\\s+not a dynamic executable$/ # ignore non-executable files\n        else\n          log.warn(log_key) do\n            \"Line did not match for #{current_library}\\n#{line}\"\n          end\n        end\n      end\n\n      bad_libs\n    end", "code_tokens": ["def", "health_check_ldd", "regexp_ends", "=", "\".*(\"", "+", "IGNORED_ENDINGS", ".", "map", "{", "|", "e", "|", "e", ".", "gsub", "(", "/", "\\.", "/", ",", "'\\.'", ")", "}", ".", "join", "(", "\"|\"", ")", "+", "\")$\"", "regexp_patterns", "=", "IGNORED_PATTERNS", ".", "map", "{", "|", "e", "|", "\".*\"", "+", "e", ".", "gsub", "(", "/", "\\/", "/", ",", "'\\/'", ")", "+", "\".*\"", "}", ".", "join", "(", "\"|\"", ")", "regexp", "=", "regexp_ends", "+", "\"|\"", "+", "regexp_patterns", "current_library", "=", "nil", "bad_libs", "=", "{", "}", "read_shared_libs", "(", "\"find #{project.install_dir}/ -type f -regextype posix-extended ! -regex '#{regexp}' | xargs ldd\"", ")", "do", "|", "line", "|", "case", "line", "when", "/", "/", "current_library", "=", "Regexp", ".", "last_match", "[", "1", "]", "log", ".", "debug", "(", "log_key", ")", "{", "\"Analyzing dependencies for #{current_library}\"", "}", "when", "/", "\\s", "\\=", "\\>", "\\s", "\\(", "\\)", "/", "name", "=", "Regexp", ".", "last_match", "[", "1", "]", "linked", "=", "Regexp", ".", "last_match", "[", "2", "]", "bad_libs", "=", "check_for_bad_library", "(", "bad_libs", ",", "current_library", ",", "name", ",", "linked", ")", "when", "/", "\\s", "\\(", "\\)", "/", "next", "when", "/", "\\s", "/", "next", "when", "/", "\\s", "/", "next", "when", "/", "\\s", "/", "next", "when", "/", "\\s", "/", "next", "when", "/", "\\s", "/", "# ignore non-executable files", "else", "log", ".", "warn", "(", "log_key", ")", "do", "\"Line did not match for #{current_library}\\n#{line}\"", "end", "end", "end", "bad_libs", "end"], "docstring": "Run healthchecks against ldd.\n\n @return [Hash<String, Hash<String, Hash<String, Int>>>]\n   the bad libraries (library_name -> dependency_name -> satisfied_lib_path -> count)", "docstring_tokens": ["Run", "healthchecks", "against", "ldd", "."], "sha": "968307c129ee54416f5a4d07ca8f8ca2d2b12825", "url": "https://github.com/chef/omnibus/blob/968307c129ee54416f5a4d07ca8f8ca2d2b12825/lib/omnibus/health_check.rb#L332-L368"}, "target": "Run healthchecks against ldd .", "arguments": {"gen_args_0": {"arg_0": "def health_check_ldd regexp_ends = \".*(\" + IGNORED_ENDINGS . map { | e | e . gsub ( / \\. / , '\\.' ) } . join ( \"|\" ) + \")$\" regexp_patterns = IGNORED_PATTERNS . map { | e | \".*\" + e . gsub ( / \\/ / , '\\/' ) + \".*\" } . join ( \"|\" ) regexp = regexp_ends + \"|\" + regexp_patterns current_library = nil bad_libs = { } read_shared_libs ( \"find #{project.install_dir}/ -type f -regextype posix-extended ! -regex '#{regexp}' | xargs ldd\" ) do | line | case line when / / current_library = Regexp . last_match [ 1 ] log . debug ( log_key ) { \"Analyzing dependencies for #{current_library}\" } when / \\s \\= \\> \\s \\( \\) / name = Regexp . last_match [ 1 ] linked = Regexp . last_match [ 2 ] bad_libs = check_for_bad_library ( bad_libs , current_library , name , linked ) when / \\s \\( \\) / next when / \\s / next when / \\s / next when / \\s / next when / \\s / next when / \\s / # ignore non-executable files else log . warn ( log_key ) do \"Line did not match for #{current_library}\\n#{line}\" end end end bad_libs end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def check_for_bad_library ( bad_libs , current_"]], "filtered_resps": ["def check_for_bad_library ( bad_libs , current_"], "doc_hash": "c3d8e178c56143e2ca856312c53bbd60e6584bf824964a1b9533b4de411f82fe", "prompt_hash": "63480b498bb105ae9ac64c879d4f69c014d79264b8ea976c3155276ca3c7b21a", "target_hash": "f54b57bd52dbdae8fd4bc4609350171d73dcb6443926d749761289a0aba7e272", "smoothed_bleu_4": 8.585639149516246e-77}
{"doc_id": 15, "doc": {"id": 15, "repo": "chef/omnibus", "path": "lib/omnibus/health_check.rb", "func_name": "Omnibus.HealthCheck.read_shared_libs", "original_string": "def read_shared_libs(command)\n      cmd = shellout(command)\n      cmd.stdout.each_line do |line|\n        yield line\n      end\n    end", "language": "ruby", "code": "def read_shared_libs(command)\n      cmd = shellout(command)\n      cmd.stdout.each_line do |line|\n        yield line\n      end\n    end", "code_tokens": ["def", "read_shared_libs", "(", "command", ")", "cmd", "=", "shellout", "(", "command", ")", "cmd", ".", "stdout", ".", "each_line", "do", "|", "line", "|", "yield", "line", "end", "end"], "docstring": "Execute the given command, yielding each line.\n\n @param [String] command\n   the command to execute\n @yield [String]\n   each line", "docstring_tokens": ["Execute", "the", "given", "command", "yielding", "each", "line", "."], "sha": "968307c129ee54416f5a4d07ca8f8ca2d2b12825", "url": "https://github.com/chef/omnibus/blob/968307c129ee54416f5a4d07ca8f8ca2d2b12825/lib/omnibus/health_check.rb#L402-L407"}, "target": "Execute the given command yielding each line .", "arguments": {"gen_args_0": {"arg_0": "def read_shared_libs ( command ) cmd = shellout ( command ) cmd . stdout . each_line do | line | yield line end end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def shellout ( command )\n  output = []\n  IO"]], "filtered_resps": ["def shellout ( command )\n  output = []\n  IO"], "doc_hash": "6438bd116c81082c32bf01500f00e9bc97f0957294d545aac8b056846fec5eed", "prompt_hash": "0948191d7de39abce82f2d0270fc82f5afacbf6eac7b4b4c4ecb868365d7894e", "target_hash": "c1b09834a71f9da4a2b33199f993edf4461f11eaa73da98a720c96263a0a2be9", "smoothed_bleu_4": 10.855926040543839}
{"doc_id": 16, "doc": {"id": 16, "repo": "chef/omnibus", "path": "lib/omnibus/health_check.rb", "func_name": "Omnibus.HealthCheck.check_for_bad_library", "original_string": "def check_for_bad_library(bad_libs, current_library, name, linked)\n      safe = nil\n\n      whitelist_libs = case Ohai[\"platform\"]\n                       when \"arch\"\n                         ARCH_WHITELIST_LIBS\n                       when \"mac_os_x\"\n                         MAC_WHITELIST_LIBS\n                       when \"solaris2\"\n                         SOLARIS_WHITELIST_LIBS\n                       when \"smartos\"\n                         SMARTOS_WHITELIST_LIBS\n                       when \"freebsd\"\n                         FREEBSD_WHITELIST_LIBS\n                       when \"aix\"\n                         AIX_WHITELIST_LIBS\n                       else\n                         WHITELIST_LIBS\n                       end\n\n      whitelist_libs.each do |reg|\n        safe ||= true if reg.match(name)\n      end\n\n      whitelist_files.each do |reg|\n        safe ||= true if reg.match(current_library)\n      end\n\n      log.debug(log_key) { \"  --> Dependency: #{name}\" }\n      log.debug(log_key) { \"  --> Provided by: #{linked}\" }\n\n      if !safe && linked !~ Regexp.new(project.install_dir)\n        log.debug(log_key) { \"    -> FAILED: #{current_library} has unsafe dependencies\" }\n        bad_libs[current_library] ||= {}\n        bad_libs[current_library][name] ||= {}\n        if bad_libs[current_library][name].key?(linked)\n          bad_libs[current_library][name][linked] += 1\n        else\n          bad_libs[current_library][name][linked] = 1\n        end\n      else\n        log.debug(log_key) { \"    -> PASSED: #{name} is either whitelisted or safely provided.\" }\n      end\n\n      bad_libs\n    end", "language": "ruby", "code": "def check_for_bad_library(bad_libs, current_library, name, linked)\n      safe = nil\n\n      whitelist_libs = case Ohai[\"platform\"]\n                       when \"arch\"\n                         ARCH_WHITELIST_LIBS\n                       when \"mac_os_x\"\n                         MAC_WHITELIST_LIBS\n                       when \"solaris2\"\n                         SOLARIS_WHITELIST_LIBS\n                       when \"smartos\"\n                         SMARTOS_WHITELIST_LIBS\n                       when \"freebsd\"\n                         FREEBSD_WHITELIST_LIBS\n                       when \"aix\"\n                         AIX_WHITELIST_LIBS\n                       else\n                         WHITELIST_LIBS\n                       end\n\n      whitelist_libs.each do |reg|\n        safe ||= true if reg.match(name)\n      end\n\n      whitelist_files.each do |reg|\n        safe ||= true if reg.match(current_library)\n      end\n\n      log.debug(log_key) { \"  --> Dependency: #{name}\" }\n      log.debug(log_key) { \"  --> Provided by: #{linked}\" }\n\n      if !safe && linked !~ Regexp.new(project.install_dir)\n        log.debug(log_key) { \"    -> FAILED: #{current_library} has unsafe dependencies\" }\n        bad_libs[current_library] ||= {}\n        bad_libs[current_library][name] ||= {}\n        if bad_libs[current_library][name].key?(linked)\n          bad_libs[current_library][name][linked] += 1\n        else\n          bad_libs[current_library][name][linked] = 1\n        end\n      else\n        log.debug(log_key) { \"    -> PASSED: #{name} is either whitelisted or safely provided.\" }\n      end\n\n      bad_libs\n    end", "code_tokens": ["def", "check_for_bad_library", "(", "bad_libs", ",", "current_library", ",", "name", ",", "linked", ")", "safe", "=", "nil", "whitelist_libs", "=", "case", "Ohai", "[", "\"platform\"", "]", "when", "\"arch\"", "ARCH_WHITELIST_LIBS", "when", "\"mac_os_x\"", "MAC_WHITELIST_LIBS", "when", "\"solaris2\"", "SOLARIS_WHITELIST_LIBS", "when", "\"smartos\"", "SMARTOS_WHITELIST_LIBS", "when", "\"freebsd\"", "FREEBSD_WHITELIST_LIBS", "when", "\"aix\"", "AIX_WHITELIST_LIBS", "else", "WHITELIST_LIBS", "end", "whitelist_libs", ".", "each", "do", "|", "reg", "|", "safe", "||=", "true", "if", "reg", ".", "match", "(", "name", ")", "end", "whitelist_files", ".", "each", "do", "|", "reg", "|", "safe", "||=", "true", "if", "reg", ".", "match", "(", "current_library", ")", "end", "log", ".", "debug", "(", "log_key", ")", "{", "\"  --> Dependency: #{name}\"", "}", "log", ".", "debug", "(", "log_key", ")", "{", "\"  --> Provided by: #{linked}\"", "}", "if", "!", "safe", "&&", "linked", "!~", "Regexp", ".", "new", "(", "project", ".", "install_dir", ")", "log", ".", "debug", "(", "log_key", ")", "{", "\"    -> FAILED: #{current_library} has unsafe dependencies\"", "}", "bad_libs", "[", "current_library", "]", "||=", "{", "}", "bad_libs", "[", "current_library", "]", "[", "name", "]", "||=", "{", "}", "if", "bad_libs", "[", "current_library", "]", "[", "name", "]", ".", "key?", "(", "linked", ")", "bad_libs", "[", "current_library", "]", "[", "name", "]", "[", "linked", "]", "+=", "1", "else", "bad_libs", "[", "current_library", "]", "[", "name", "]", "[", "linked", "]", "=", "1", "end", "else", "log", ".", "debug", "(", "log_key", ")", "{", "\"    -> PASSED: #{name} is either whitelisted or safely provided.\"", "}", "end", "bad_libs", "end"], "docstring": "Check the given path and library for \"bad\" libraries.\n\n @param [Hash<String, Hash<String, Hash<String, Int>>>]\n   the bad libraries (library_name -> dependency_name -> satisfied_lib_path -> count)\n @param [String]\n   the library being analyzed\n @param [String]\n   dependency library name\n @param [String]\n   actual path of library satisfying the dependency\n\n @return the modified bad_library hash", "docstring_tokens": ["Check", "the", "given", "path", "and", "library", "for", "bad", "libraries", "."], "sha": "968307c129ee54416f5a4d07ca8f8ca2d2b12825", "url": "https://github.com/chef/omnibus/blob/968307c129ee54416f5a4d07ca8f8ca2d2b12825/lib/omnibus/health_check.rb#L423-L468"}, "target": "Check the given path and library for bad libraries .", "arguments": {"gen_args_0": {"arg_0": "def check_for_bad_library ( bad_libs , current_library , name , linked ) safe = nil whitelist_libs = case Ohai [ \"platform\" ] when \"arch\" ARCH_WHITELIST_LIBS when \"mac_os_x\" MAC_WHITELIST_LIBS when \"solaris2\" SOLARIS_WHITELIST_LIBS when \"smartos\" SMARTOS_WHITELIST_LIBS when \"freebsd\" FREEBSD_WHITELIST_LIBS when \"aix\" AIX_WHITELIST_LIBS else WHITELIST_LIBS end whitelist_libs . each do | reg | safe ||= true if reg . match ( name ) end whitelist_files . each do | reg | safe ||= true if reg . match ( current_library ) end log . debug ( log_key ) { \" --> Dependency: #{name}\" } log . debug ( log_key ) { \" --> Provided by: #{linked}\" } if ! safe && linked !~ Regexp . new ( project . install_dir ) log . debug ( log_key ) { \" -> FAILED: #{current_library} has unsafe dependencies\" } bad_libs [ current_library ] ||= { } bad_libs [ current_library ] [ name ] ||= { } if bad_libs [ current_library ] [ name ] . key? ( linked ) bad_libs [ current_library ] [ name ] [ linked ] += 1 else bad_libs [ current_library ] [ name ] [ linked ] = 1 end else log . debug ( log_key ) { \" -> PASSED: #{name} is either whitelisted or safely provided.\" } end bad_libs end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [[""]], "filtered_resps": [""], "doc_hash": "0d2f3d49d34df12b444fbfdcb1ead4f5fa0b7013588c57bbf4dc3d8939a0c035", "prompt_hash": "d2efffc4bc005716e8299c1a6dbf21a9facad6caafe95e2e6e95acb919a990aa", "target_hash": "864fcc908ed6b3582592db5901afc49dcaec445f8e6b641521ff8d05335bf545", "smoothed_bleu_4": 0.004539992976248485}
{"doc_id": 17, "doc": {"id": 17, "repo": "chef/omnibus", "path": "lib/omnibus/digestable.rb", "func_name": "Omnibus.Digestable.digest", "original_string": "def digest(path, type = :md5)\n      digest = digest_from_type(type)\n\n      update_with_file_contents(digest, path)\n      digest.hexdigest\n    end", "language": "ruby", "code": "def digest(path, type = :md5)\n      digest = digest_from_type(type)\n\n      update_with_file_contents(digest, path)\n      digest.hexdigest\n    end", "code_tokens": ["def", "digest", "(", "path", ",", "type", "=", ":md5", ")", "digest", "=", "digest_from_type", "(", "type", ")", "update_with_file_contents", "(", "digest", ",", "path", ")", "digest", ".", "hexdigest", "end"], "docstring": "Calculate the digest of the file at the given path. Files are read in\n binary chunks to prevent Ruby from exploding.\n\n @param [String] path\n   the path of the file to digest\n @param [Symbol] type\n   the type of digest to use\n\n @return [String]\n   the hexdigest of the file at the path", "docstring_tokens": ["Calculate", "the", "digest", "of", "the", "file", "at", "the", "given", "path", ".", "Files", "are", "read", "in", "binary", "chunks", "to", "prevent", "Ruby", "from", "exploding", "."], "sha": "968307c129ee54416f5a4d07ca8f8ca2d2b12825", "url": "https://github.com/chef/omnibus/blob/968307c129ee54416f5a4d07ca8f8ca2d2b12825/lib/omnibus/digestable.rb#L40-L45"}, "target": "Calculate the digest of the file at the given path . Files are read in binary chunks to prevent Ruby from exploding .", "arguments": {"gen_args_0": {"arg_0": "def digest ( path , type = :md5 ) digest = digest_from_type ( type ) update_with_file_contents ( digest , path ) digest . hexdigest end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def digest_from_type ( type ) case type when :md5 then"]], "filtered_resps": ["def digest_from_type ( type ) case type when :md5 then"], "doc_hash": "b8f7b48dab475865a4092c1828e0f56df4770bd7ab25f787f50459851d860dd6", "prompt_hash": "fb656258416fd46f6744d49f0bacdccb364744d4cc10e7ae04b6896277e91b0f", "target_hash": "bde29d9ddb570095ffb880487d73a337de167b0f98e51742fa6aab28da362cb3", "smoothed_bleu_4": 5.070453233664989}
{"doc_id": 18, "doc": {"id": 18, "repo": "chef/omnibus", "path": "lib/omnibus/digestable.rb", "func_name": "Omnibus.Digestable.update_with_file_contents", "original_string": "def update_with_file_contents(digest, filename)\n      File.open(filename) do |io|\n        while (chunk = io.read(1024 * 8))\n          digest.update(chunk)\n        end\n      end\n    end", "language": "ruby", "code": "def update_with_file_contents(digest, filename)\n      File.open(filename) do |io|\n        while (chunk = io.read(1024 * 8))\n          digest.update(chunk)\n        end\n      end\n    end", "code_tokens": ["def", "update_with_file_contents", "(", "digest", ",", "filename", ")", "File", ".", "open", "(", "filename", ")", "do", "|", "io", "|", "while", "(", "chunk", "=", "io", ".", "read", "(", "1024", "*", "8", ")", ")", "digest", ".", "update", "(", "chunk", ")", "end", "end", "end"], "docstring": "Update the digest with the given contents of the file, reading in small\n chunks to reduce memory. This method will update the given +digest+\n parameter, but returns nothing.\n\n @param [Digest] digest\n   the digest to update\n @param [String] filename\n   the path to the file on disk to read\n\n @return [void]", "docstring_tokens": ["Update", "the", "digest", "with", "the", "given", "contents", "of", "the", "file", "reading", "in", "small", "chunks", "to", "reduce", "memory", ".", "This", "method", "will", "update", "the", "given", "+", "digest", "+", "parameter", "but", "returns", "nothing", "."], "sha": "968307c129ee54416f5a4d07ca8f8ca2d2b12825", "url": "https://github.com/chef/omnibus/blob/968307c129ee54416f5a4d07ca8f8ca2d2b12825/lib/omnibus/digestable.rb#L122-L128"}, "target": "Update the digest with the given contents of the file reading in small chunks to reduce memory . This method will update the given + digest + parameter but returns nothing .", "arguments": {"gen_args_0": {"arg_0": "def update_with_file_contents ( digest , filename ) File . open ( filename ) do | io | while ( chunk = io . read ( 1024 * 8 ) ) digest . update ( chunk ) end end end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [["def update_with_file_name ( digest , filename ) File . open"]], "filtered_resps": ["def update_with_file_name ( digest , filename ) File . open"], "doc_hash": "88c2e71bf141d5162fd59241c1f30ffa59e71f87079ec32eb6a8e61f25edf126", "prompt_hash": "3e760a2e985531e9dd0656db404cb694df7a0807d819dc64b085f32481d710f5", "target_hash": "1fc0affb7db415c4aafc51795d0201db0eed636e21e1e4d6b9f44ddca87d71b2", "smoothed_bleu_4": 3.8315980044362785}
{"doc_id": 19, "doc": {"id": 19, "repo": "chef/omnibus", "path": "lib/omnibus/publisher.rb", "func_name": "Omnibus.Publisher.packages", "original_string": "def packages\n      @packages ||= begin\n        publish_packages = Array.new\n        build_packages   = FileSyncer.glob(@pattern).map { |path| Package.new(path) }\n\n        if @options[:platform_mappings]\n          # the platform map is a simple hash with publish to build platform mappings\n          @options[:platform_mappings].each_pair do |build_platform, publish_platforms|\n            # Splits `ubuntu-12.04` into `ubuntu` and `12.04`\n            build_platform, build_platform_version = build_platform.rpartition(\"-\") - %w{ - }\n\n            # locate the package for the build platform\n            packages = build_packages.select do |p|\n              p.metadata[:platform] == build_platform &&\n                p.metadata[:platform_version] == build_platform_version\n            end\n\n            if packages.empty?\n              log.warn(log_key) do\n                \"Could not locate a package for build platform #{build_platform}-#{build_platform_version}. \" \\\n                \"Publishing will be skipped for: #{publish_platforms.join(', ')}\"\n              end\n            end\n\n            publish_platforms.each do |publish_platform|\n              publish_platform, publish_platform_version = publish_platform.rpartition(\"-\") - %w{ - }\n\n              packages.each do |p|\n                # create a copy of our package before mucking with its metadata\n                publish_package  = p.dup\n                publish_metadata = p.metadata.dup.to_hash\n\n                # override the platform and platform version in the metadata\n                publish_metadata[:platform]         = publish_platform\n                publish_metadata[:platform_version] = publish_platform_version\n\n                # Set the updated metadata on the package object\n                publish_package.metadata = Metadata.new(publish_package, publish_metadata)\n\n                publish_packages << publish_package\n              end\n            end\n          end\n        else\n          publish_packages.concat(build_packages)\n        end\n\n        if publish_packages.empty?\n          log.info(log_key) { \"No packages found, skipping publish\" }\n        end\n\n        publish_packages\n      end\n    end", "language": "ruby", "code": "def packages\n      @packages ||= begin\n        publish_packages = Array.new\n        build_packages   = FileSyncer.glob(@pattern).map { |path| Package.new(path) }\n\n        if @options[:platform_mappings]\n          # the platform map is a simple hash with publish to build platform mappings\n          @options[:platform_mappings].each_pair do |build_platform, publish_platforms|\n            # Splits `ubuntu-12.04` into `ubuntu` and `12.04`\n            build_platform, build_platform_version = build_platform.rpartition(\"-\") - %w{ - }\n\n            # locate the package for the build platform\n            packages = build_packages.select do |p|\n              p.metadata[:platform] == build_platform &&\n                p.metadata[:platform_version] == build_platform_version\n            end\n\n            if packages.empty?\n              log.warn(log_key) do\n                \"Could not locate a package for build platform #{build_platform}-#{build_platform_version}. \" \\\n                \"Publishing will be skipped for: #{publish_platforms.join(', ')}\"\n              end\n            end\n\n            publish_platforms.each do |publish_platform|\n              publish_platform, publish_platform_version = publish_platform.rpartition(\"-\") - %w{ - }\n\n              packages.each do |p|\n                # create a copy of our package before mucking with its metadata\n                publish_package  = p.dup\n                publish_metadata = p.metadata.dup.to_hash\n\n                # override the platform and platform version in the metadata\n                publish_metadata[:platform]         = publish_platform\n                publish_metadata[:platform_version] = publish_platform_version\n\n                # Set the updated metadata on the package object\n                publish_package.metadata = Metadata.new(publish_package, publish_metadata)\n\n                publish_packages << publish_package\n              end\n            end\n          end\n        else\n          publish_packages.concat(build_packages)\n        end\n\n        if publish_packages.empty?\n          log.info(log_key) { \"No packages found, skipping publish\" }\n        end\n\n        publish_packages\n      end\n    end", "code_tokens": ["def", "packages", "@packages", "||=", "begin", "publish_packages", "=", "Array", ".", "new", "build_packages", "=", "FileSyncer", ".", "glob", "(", "@pattern", ")", ".", "map", "{", "|", "path", "|", "Package", ".", "new", "(", "path", ")", "}", "if", "@options", "[", ":platform_mappings", "]", "# the platform map is a simple hash with publish to build platform mappings", "@options", "[", ":platform_mappings", "]", ".", "each_pair", "do", "|", "build_platform", ",", "publish_platforms", "|", "# Splits `ubuntu-12.04` into `ubuntu` and `12.04`", "build_platform", ",", "build_platform_version", "=", "build_platform", ".", "rpartition", "(", "\"-\"", ")", "-", "%w{", "-", "}", "# locate the package for the build platform", "packages", "=", "build_packages", ".", "select", "do", "|", "p", "|", "p", ".", "metadata", "[", ":platform", "]", "==", "build_platform", "&&", "p", ".", "metadata", "[", ":platform_version", "]", "==", "build_platform_version", "end", "if", "packages", ".", "empty?", "log", ".", "warn", "(", "log_key", ")", "do", "\"Could not locate a package for build platform #{build_platform}-#{build_platform_version}. \"", "\"Publishing will be skipped for: #{publish_platforms.join(', ')}\"", "end", "end", "publish_platforms", ".", "each", "do", "|", "publish_platform", "|", "publish_platform", ",", "publish_platform_version", "=", "publish_platform", ".", "rpartition", "(", "\"-\"", ")", "-", "%w{", "-", "}", "packages", ".", "each", "do", "|", "p", "|", "# create a copy of our package before mucking with its metadata", "publish_package", "=", "p", ".", "dup", "publish_metadata", "=", "p", ".", "metadata", ".", "dup", ".", "to_hash", "# override the platform and platform version in the metadata", "publish_metadata", "[", ":platform", "]", "=", "publish_platform", "publish_metadata", "[", ":platform_version", "]", "=", "publish_platform_version", "# Set the updated metadata on the package object", "publish_package", ".", "metadata", "=", "Metadata", ".", "new", "(", "publish_package", ",", "publish_metadata", ")", "publish_packages", "<<", "publish_package", "end", "end", "end", "else", "publish_packages", ".", "concat", "(", "build_packages", ")", "end", "if", "publish_packages", ".", "empty?", "log", ".", "info", "(", "log_key", ")", "{", "\"No packages found, skipping publish\"", "}", "end", "publish_packages", "end", "end"], "docstring": "Create a new publisher from the given pattern.\n\n @param [String] pattern\n   the path/pattern of the release artifact(s)\n\n @param [Hash] options\n   the list of options passed to the publisher\n @option options [Hash] :platform_mappings A simple\n   mapping of build to publish platform(s)\n @example\n   {\n     'ubuntu-10.04' => [\n       'ubuntu-10.04',\n       'ubuntu-12.04',\n       'ubuntu-14.04',\n     ],\n   }\n\n\n The list of packages that match the pattern in the initializer.\n\n @return [Array<String>]", "docstring_tokens": ["Create", "a", "new", "publisher", "from", "the", "given", "pattern", "."], "sha": "968307c129ee54416f5a4d07ca8f8ca2d2b12825", "url": "https://github.com/chef/omnibus/blob/968307c129ee54416f5a4d07ca8f8ca2d2b12825/lib/omnibus/publisher.rb#L70-L123"}, "target": "Create a new publisher from the given pattern .", "arguments": {"gen_args_0": {"arg_0": "def packages @packages ||= begin publish_packages = Array . new build_packages = FileSyncer . glob ( @pattern ) . map { | path | Package . new ( path ) } if @options [ :platform_mappings ] # the platform map is a simple hash with publish to build platform mappings @options [ :platform_mappings ] . each_pair do | build_platform , publish_platforms | # Splits `ubuntu-12.04` into `ubuntu` and `12.04` build_platform , build_platform_version = build_platform . rpartition ( \"-\" ) - %w{ - } # locate the package for the build platform packages = build_packages . select do | p | p . metadata [ :platform ] == build_platform && p . metadata [ :platform_version ] == build_platform_version end if packages . empty? log . warn ( log_key ) do \"Could not locate a package for build platform #{build_platform}-#{build_platform_version}. \" \"Publishing will be skipped for: #{publish_platforms.join(', ')}\" end end publish_platforms . each do | publish_platform | publish_platform , publish_platform_version = publish_platform . rpartition ( \"-\" ) - %w{ - } packages . each do | p | # create a copy of our package before mucking with its metadata publish_package = p . dup publish_metadata = p . metadata . dup . to_hash # override the platform and platform version in the metadata publish_metadata [ :platform ] = publish_platform publish_metadata [ :platform_version ] = publish_platform_version # Set the updated metadata on the package object publish_package . metadata = Metadata . new ( publish_package , publish_metadata ) publish_packages << publish_package end end end else publish_packages . concat ( build_packages ) end if publish_packages . empty? log . info ( log_key ) { \"No packages found, skipping publish\" } end publish_packages end end", "arg_1": {"num_beams": 10, "max_gen_toks": 128, "until": ["</s>"]}}}, "resps": [[""]], "filtered_resps": [""], "doc_hash": "01a9fafdd269ad488e69d52aac00f0837924452c819b43436e7b3438a1df8b99", "prompt_hash": "7ccd601ef7354030e62f987823aad676a770bd908605cd541dadfb707b72293e", "target_hash": "b7f3d489d3189088f11124756d4f6fe4b9c81a2bdfe1937202a338b3f427ef78", "smoothed_bleu_4": 0.012340980408667957}
